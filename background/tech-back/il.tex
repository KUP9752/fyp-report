\section{Imitation Learning (IL)}
  While traditional Reinforcement learning models focus on interactions with the environment to optimise a reward signal to find an optimal policy, these can be sample inefficient, or very challenging in high-dimensional tasks. So, another approach we can take in teaching robots is Imitation Learning (IL) where an agent learns directly from expert demonstrations \cite{attia2018globaloverviewimitationlearning}, potentially bypassing the need of extensive exploration such an adjacent RL system would need.

  Imitation Learning, or as some literature refers: \emph{Learning from Demonstrations (LfD)} \cite{ARGALL2009469}; is a form of \emph{Supervised Learning} \cite{hastie2009overview,cunningham2008supervised}, where the model is presented with labelled expert demonstrations and the model learns from those.
  
  The roots of this idea comes from humans and animals \cite{bakker1996robot} learning from observations to copy movements from carers \emph{(supervisors)} for survival. 
  \todo{not sure about this para move or remove}

  This approach is beneficial for systems where the exploration of the action space is dangerous, expensive or inefficient.\todo{examples?, autonomous driving, healthcaare applications, robot manipulation ... }

  We can broadly classify IL into two main categories, \textbf{Behavioural Cloning} and \textbf{Inverse Reinforcement Learning} each addressing the challenge of learning from demonstration slightly differently.


\subsection{Behavioural Cloning (BC)}
 Behavioural Cloning (BC) approaches achieve this goal by  mimicking the action given to it by training a model that predicts the function between the state and the actions taken  \cite{pomerlau1991neco.1991.3.1.88, ross2011reductionimitationlearningstructured}. The expert behaviour will be recorded as a set of demonstrations (or \emph{trajectories}) $\tau^* = \lbrace(s_i^*, a_i^*)\rbrace_{i = 0}^N$ (for $N$ total demonstrations). Where the demonstrations are state-action pairs and \emph{$*$} meaning \emph{expert or optimal}.
 Then using supervised learning methods and treating the demonstrations as the training \todo{and test?} data we can predict a policy $\pi_\theta\left(a | s\right)$ along with a loss function $L \left( a_i^*, \pi_\theta\left(s_i^*\right) \right)$ \todo{loss function is typically MSE or cross-entropy loss for discrete actions} which we can minimise to extract the optimal policy. 

 However, one big downside of this method is that the demonstrations are heavily coupled to the model, meaning slight deviations from the given behaviour in the learnt optimal policy might lead the agent into unfamiliar states. Which then can lead to compounding errors known as covariate shift, and should be controlled for stability of learning \cite{mehta2024stablebccontrollingcovariateshift}.

 Another issue a method like this faces is the adaptation problem. The model will not adapt to its environment but solely copy behaviour. As there is no reward signal optimise for, this means that unlike RL approaches, it cannot adapt to the environment. Adding on, if the \emph{expert} demonstrations are of low quality or have mistakes, the agent might learn these mistakes as a part of its policy, and never be able to correct itself due to the missing context.


\subsection{Inverse Reinforcement Learning (IRL)}

\todo{Hybrid approaches exists such as Offline RL learning from datasets, and DAgger (Dataset Aggregation) Il with corrective RL-style updates either mention in related work or here later on}