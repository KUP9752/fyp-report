\section{Reinforcement Learning (RL)}
  One of the tried and tested methods of end-to-end learning approaches is a branch under machine learning called \emph{Reinforcement Learning (RL)}. \todo{is it a branch under machine learning?}
  \missingfigure{add the classic agent state reward action diagram}

  RL's main focus is training robots, which are called \emph{agents} in making decisions by interacting with the environment. The key objective is to teach a \emph{policy} to the agent that maximises the overall reward -usually defined by the task and involves a \emph{teacher}. The agent will explore, the possibly actions it can take through trail and error, while learning from the feedback given to it by the reward signal and its environment.

  One of the differentiating factors of RL from classical machine learning paradigms is that the feedback is not instantaneous and sequences of decisions influence the subsequent data and signals given to agent \cite{silver2015}.

  \subsection{Mathematical Foundations}

  In scenarios involving autonomous acting, the capability of a an robot to reason and navigate complex problems or dynamic environments plays a central role. So in any RL system the agent must make a sequence of decisions that impact later outcomes. Markov Decision Processes (MDPs) provide a mathematical framework for modelling decisions in environments where the probabilistic outcomes are influenced by actions of such an agent. A formally defined MDP will have the following parts \cite{silver2015}:

\subsection{Markov Decision Processes}
  \subsubsection{State, S}
    The system must be able to process all different configurations of the environment. So the state will encapsulate the surroundings through raw sensory inputs (e.g. images, force sensors) in a high-level representation and will capture all relevant information available \cite{Sutton1998} which might be needed to make an informed decision at a particular time step.

  \subsubsection{The Markov Property}
    A state $S_t$ is \emph{Markov} if and only if:
    \[
      \mathbb{P} \left[S_{t+1} \mid S_t\right] = \mathbb{P}\left[ S_{t+1} \mid S_1, \ldots, S_t\right]
    \]

   % \todo{maybe make this a lemma type table??}
    This property ensures that all relevant information from the history is captured within the state. So once the state is knows the past states can be discarded. Making the current state a sufficient statistic for the future \cite{silver2015}.

  \subsubsection{State Transition Matrix, P}
    This matrix defines the transition probabilities from all states $s$ to all successor states $s'$, so:
    
    \[ P_{ss'} = \mathbb{P} \left[S_{t+1} = s'  \mid S_t = s\right]\] 
    and 
    
    \[ P =
    \begin{bmatrix}
      P_{11} & \cdots & P_{1n} \\ 
      \vdots & & \vdots\\
      P_{n1} & \cdots & P_{nn}
    \end{bmatrix}
    \]
    where each row of the matrix sums up to 1, due to the nature of probabilities. A tuple of a set of states and a transition matrix/function \(\llangle S, P \rrangle\) make up a \textbf{Markov Process} (or Markov Chain)

  \subsubsection{Actions, A}
    This is the set of all possible actions that are available to the robot in each state. Depending on the context of the task, actions can be discrete or continuous.

  \subsubsection{Reward Function, R}
    This is the scalar feedback signal. It ensures that the agent's learning is based on steps it is taking over time, so $R_t$ is how well the agent is doing at step $t$. It is defined as:

    \[R_s = \mathbb{E} \left[R_{t+1} \mid S_t = s\right]\]
    
    This allows the robot to eventually converge to a solution that maximises the cumulative rewards (the \emph{returns}) for the actions it has taken. 
  
  \subsubsection{Discount Factor, $\gamma$}
    This is mechanism to control the importance of future rewards. Sampled as: \(\gamma \in \left[0, 1\right]\), means that immediate reward is prioritised and while reward form longer sequence of actions decays, avoiding infinite cycles in Markov Chains.

    Combining the reward function and the discount factor we can defined the \emph{return}, $G_t$ as the total discounted reward from time-step $t$:

    \[ 
    \begin{aligned}
      G_t &= R_{t+1} + \gamma R_{t+2} + \ldots \\ 
      &= \sum_{k=0}^{\inf}\gamma^k R_{t+k+1} 
    \end{aligned}
    \]
    
    Therefore, a \emph{Markov Decision Process} is a tuple \(\langle S, A, P, R, \gamma \rangle\) as the parts are defined above.
    
    
  \subsubsection{Policy, $\pi$}
    The higher-level goal of any RL system is to learn an optimal policy \(\pi \left( a \mid s\right) = \mathbb{P} \left[A_t = a \mid S_t = s\right]\) which aims to maximise the return. Policies fully define the behaviour of the agent. As seen by the function's type \(\pi: S \rightarrow A \) they only depend on the current state and are time independent \( A_t \sim \pi\left( \cdot \mid S_t\right), \forall t > 0 \)
    
  \subsubsection{Value Functions}
    On top of these we define two value function, \emph{state-value}: the expected return starting from state $s$, and then following policy $\pi$:

    \[ v_\pi \left(s\right) = \mathbb{E} = \left[G_t \mid S_t = s\right]\]

    and the \emph{action-value} function, which is the expected return starting from state $s$, taking action $a$, and then continuing a policy $\pi$:

    \[ q_\pi \left(s, a\right) = \mathbb{E} \left[ G_t \mid S_t = s, A_t = a\right]\]


    while the optimal versions can be defined as:
    \[v_* \left(s\right) = \underset{\pi}{\max} \ v_\pi \left(s\right)\]

    \[q_* \left(s, a\right) = \underset{\pi}{\max} \ q_\pi \left(s, a\right)\]
    
    The optimal means the best possible performance can be achieved in the MDP and it can be considered ``solved'' once we find these optimal functions.

    \todo{should I define the bellman function?}

    \subsection{Exploration and Exploitation}
    \todo{this is left for tmr man, I had enough gotta finish the model flavours}

\section{RL in Practice}
  Reinforcement learning, can be used to train a variety of agents that is not limited by physical robots. It can learn to play video-games \cite{comi2018}, automation tasks \cite{}, in natural language processing \cite{paulus2017deepreinforcedmodelabstractive}, applications  in healthcare (where RL is categorised as dynamic treatment regimes) for use in chronic diseases or critical care \cite{yu2020reinforcementlearninghealthcaresurvey} and lastly -most importantly for us- learning movement behaviours for robots \cite{}.
  \todo{add more sources and citations}
  
  A fundamental challenge in utilising RL is the constant act of balancing \textbf{exploration} and \textbf{exploration}. A trade-off must be made in the chosen model \todo{model might not be the right word here as it hints at model-based?}
  \begin{enumerate}
    \item \textbf{Exploration:}
    This is when the agent decides to \emph{explore} new actions that might potentially lead to better long-term outcomes. An issue this can cause is that the time it takes to explore all possibilities might not be feasible. But crucial to utilise in in problems with sparse reward models \cite{}
    \item \textbf{Exploitation:}
    When the agent prioritises short-term, immediate rewards by exploiting its current knowledge. For example, taking an agent playing a video game; if a high score was found the agent might be unaware that a higher score can be achieved with a different set of moves. 
  \end{enumerate}

  The trade-offs must be balanced between the two in any RL algorithm or model while considered sampling efficiencies and ease of training \cite{liu2019simpleexplorationsampleefficient}. To relate it back to a robotics example, too much exploration might lead to inefficient training and instability; while too much reliance on exploitation might lead to suboptimal behaviours in the movement of a robot executing a task. 

  Some common strategies are:
  \begin{itemize}
    \item $\epsilon$-Greedy:
    \item Decay $\epsilon$-Greedy:
    \item Upper Confidence Bound (UCB):
    \item Thompson Sampling
    \item Intrinsic Motivation (Curiosity-Driven Exploration):
    \todo{find sources and confirm and give quick details about pros/cons?}
  \end{itemize}
  
  Along with this widespread use and elemental challenges, comes differing methods of utilising the RL framework. The likes of which can be broadly classified into two types: \textbf{Model-Based} and \textbf{Model-Free}.
  
  \subsection{Model-Based RL}
  Model based approaches involve methods of creating an internal model and representation of the state the agent is interacting with. It usually involves two main steps: learning the dynamics of the model, then planning and learning within it \cite{MAL-086}. These models have underlying principals of the Markov Decision Process.
  \todo{link to markov decision process}

  Main power of this approach comes from its simulated core. As fewer real interactions can allow it to have a higher sample efficiency \cite{liu2021DRLminireview,wu23robotLearn}, meaning the amount of experience needed (mostly relating to time spent) to learn optimal policies is quite low, and good policies can be quickly learnt.
  On top of that having an underlying model essentially allows the agent to understand its surroundings better, without having to guess or learn them. This means the model can focus on learn the model and generalise better for unseen states \cite{MAL-086}.
  
  However, it also has some drawbacks. Mainly the model introducing a bias into the system, which means inadequate representations or faulty models can create policies that exploit deficiencies in these models \cite{Deisenroth2011PILCO,wang2019benchmarkingmodelbasedreinforcementlearning}, although some recent works have helped alleviate that bias by characterising the uncertainty of the learnt models \cite{kurutach2018modelensembletrustregionpolicyoptimization,chua2018deepreinforcementlearninghandful,clavera2018modelbasedreinforcementlearningmetapolicy}.

  One of the most important issues being the learning of the dynamics being coupled with the policy. This makes the agent more prone to performance local-minima, which stem from exploitation and off-learning not being fully investigated under model-based approaches (will be explained later) \todo{verify this is true and check some sources}

  \subsubsection{Applications in Robotics}
  These methods can be used for motion planning, trajectory optimisation and learning from limited interactions (few-shot learning?? \todo[color=red]{not sure abt few-shot})
    \begin{itemize}
      \item Monte Carlo Tree Search (MCTS): Used in planning based systems.
      \item Probabilistic Inference for Learning Control (PILCO): Similar uses but reduces sample complexity \cite{Deisenroth2011PILCO}.
      \item MuZero: Combines model-based approaches with deep learning. \todo[color=green]{cite here}
    \end{itemize}
  
  
  \subsection{Model-Free RL}
  This collection of schemes attempt to learn a policy directly by trial and error, without explicitly modelling the environments dynamics. Usually preferred when the environments is too complex or too costly to model.
  There are a few different variants of model-free approaches, all of them similar in the way they omit a model, but have different methods in extracting the optimal policies.
  

  \subsubsection{Value-Based}
    These techniques aim to learn the value of states (or and estimate for the value of states) and actions. So they learn the $v_\pi$ or $q_\pi$ function. which can then be used to extract the optimal policy $\pi_*$ for deciding the actions.

    Such systems are mainly used for simple navigation tasks, basic motor control and arm reaching scenarios.

    \begin{itemize}
      \item Q-Learning (Off-Policy): Learns the optimal action-value function.
      \item Deep Q-Network (DQN) (Off-Policy)
      \item SARSA (On-Policy)
      \todo[color=green]{check the correctness of the above and citations?}
    \end{itemize}

  \subsubsection{Policy-Based}
  This on the other hand learn a policy directly. Bypassing the need to learn the values of states or actions completely. This can be helpful if the state space and/or the action space are quite large. For example, if the action space was infinite, then the above approach would not be feasible as all actions must be tried to find the best, which makes directly learning the policy is the only possible approach.

  These systems are for more complex control tasks, such as dexterous limb manipulation, robot hand grasping.

    \begin{itemize}
      \item REINFORCE (On-Policy)
      \item Proximal Policy Optimisation (PPO) (On-Policy)
      \item Trust Region Policy Optimisation (TRPO) (On-Policy)
      \todo[color=green]{as before check and explain and cite}
    \end{itemize}

  \subsubsection{Actor-Teacher} 
  \todo{not sure if this is strictly model-free will see tomorrow}
  
  \subsection{Hybrid Methods}
  Mixing the two together is also a viability, where the characteristics from each can be combined benefit from different guarantees each provides \cite{qu2020combiningmodelbasedmodelfreemethods}
  \todo[color=red]{not sure about this chatgpt said: 
  4. Hybrid Approaches
  Hierarchical RL: Breaks complex tasks into sub-tasks (e.g., robot assembly tasks).
  Curriculum Learning: Trains the robot on progressively harder tasks (e.g., training a legged robot to walk before running).
  Multi-Agent RL: Used in swarm robotics and cooperative robotic tasks.}
\\\\
  On top of the model dependence, the RL approach can be of two flavours:

  \subsection{On-Policy}
  This approach means tat the agent updates its policy from data generated by the current policy. So an agent can only learn from the actions it purposefully took under its latest policy. Which can restrict improvements, but means that no outdated or stale data can influence the present time actions or learning.
  This leads to more stable learning in stochastic environments \todo[color=green]{citation?} while ensuring policy consistency.
  However, it also means that the agent becomes sample inefficient, as fresh iterations are needed to learn and sharpen the current policy. On top of that, the learning rate slows down because the agent can't use older experience (only the latest policy), slowing training.


  \subsection{Off-Policy}
  In this case, the agent can utilise past experiences, regardless of the policy generated by the. This recycling of information previously known makes the learning more efficient, as sample efficiency is increased.
  Having past experiences also means that the agent will need fewer exploration steps and less iterations because of these priors. Making this system efficient in deterministic environments \todo{not quite sure why, add here}
  This however, brings instability in stochastic environments, and the the policy diverging due to incorporating outdated and possibly uninformative (at least at the present) data. Therefore, some sort of importance of older steps should be kept track of, and maybe even decayed like the discount factor from earlier \todo{find evidence of decay, and link to earlier}
  \\\\
  Both of these aproaches can be combined with wither \textbf{Model-Based} or \textbf{Model-Free} approaches. As it will depend on the scenario and the system to fine tune and see what is required.
