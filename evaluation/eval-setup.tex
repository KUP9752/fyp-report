\section{Evaluation Setup}
The general setup included training a set of policies with their inherent hyperparameters, which will be discussed during their specific sections.

\subsection{Collected Data}
The data I chose to collect to reason about these policies are:
\begin{itemize}
  \item \textbf{Camera Type} A misnomer, indicates which sensors or combinations of sensors are being used in the policy to make decisions from \({RGB}_{ \{wrist, ~left\_shoulder, ~right\_shoulder \} }\) and \(Depth_{wrist}\).
  \item \textbf{Final Distance} (to target) all of my tasks are target centric, so final distance within the allowed episode length a measure of competence.
  \item \textbf{Minimum Distance} (to target) as above. The difference is reasoning where a policy might be falling apart, seeing the discrepancies between \emph{min} and \emph{final}
  \item \textbf{Success Rate} All the tasks had slightly differing success criteria, however this is an important measure for competency again. This was mostly measured as a count out of $n$, for $n$ being the length of the test demonstrations. The term ``test demonstrations'' means that a set of demos were recorded with the target and the environment in a fixed state. Loading these back allows us to hae a comparable test between policies and their variables.
  \item Other policy specific hyperparameters, will be discussed when relevant
\end{itemize}

\subsection{Reproducibility and Verification}
All the random aspects that can be controlled, be it \emph{numpy} and \emph{pytorch} random choices or random `live' demos. Seeds are sets for both the libraries, as well as the \verb|DataLoader| seeds being fixed for the entirety of the policies tested here.
The demos per task are pre-made and saved in the project repository. \todo{ref the final deliverables bit}. I repeated all the tests with $5$ different seeds for the data shuffling, to observe if training on a different (but controlled) ordering of the demos affects a policy.

\section{Test Environments}
Two main branches of tasks I followed in this project are grasping and occluded reaching tasks.

\subsection{Reaching with an Obstacle - \textbf{ReachObs\_Random}}

\textbf{ReachObs\_IndRandom} is not discussed here in depth, due to the similarity of the tasks, and the independent version not being any more interesting. Some results for this test can be found here\todo{add appendix}.

\subsection{Grasping}
These two grasping tasks are to assess the contextual 3D understanding of the policy: learning depth information and adapting to differing sizes of targets; as well as learning the workspace before attempting a task \todo{grasp then move, not sure if I have time for this, but If i do defo interesting}

\subsection{Grasp and Depth Understanding - \textbf{Vision\_Random}}
This task is quite similar to the depth interfacing tests from earlier.\todo{ref DI}. The main difference is that the placement of the targets are random within he workspace, for \emph{training}, \emph{control} and \emph{test} sets. The test is repeated with different sizes of targets, \textbf{normal} and \textbf{smaller}. \textbf{normal} sets the scale of the target to $1$ and \textbf{smaller} sets it to $0.5$. The training and the control sets are the same size while the test will be the the other. The reasons for this is to assess:
\begin{enumerate}
  \item The effectiveness of the configuration in solving the task it is immediately trained for
  \item Assess the information extraction from the various views by comparing the discrepancy between control and test.
\end{enumerate}
I created two separate variations of this specific task. One where the training and control set is set to be the \textbf{normal} size and we evaluate that on \textbf{smaller} blocks. The next variation is flipping this configuration: control and training are scaled down, while test is normal sized. Mainly to investigate whether the information wew train on has any affect on the policy proficiency.

\textbf{Important Note!}
The demos saved for all the configurations are tested and checked to appear correctly. However, I have realised that the specifically the \textbf{saved smaller testing demos} which is used in the place of the control or the test, would spawn three of the targets smaller than they should be. Indices  \(0, ~5, ~6\)  will spawn smaller and hence look smaller. I will remove these when evaluating the policies.

I have not figured out the reasoning for this, it seems to randomly happen, I speculate that it is an issue with how I control the sizes of targets. The target size is set during the start of an episode, so this means the \verb|Environment| object from PyRep must have initialised it wrong when the demos were being saved. This is not an isolated incident and sometimes happens while resetting the environment. However, the ideal part is if it is encoded in the saved demos, it is at least controlled. The \textbf{smaller} training set is confirmed to spawn correctly. The evaluation of the results will consider this.

\subsection{Carrying the Target - \textbf{Grasp\_ThenMove}}\todo{havent run this yet, might remove it depending on what I will do today}

\section{Parameters}

\subsection{Agent Parameters}
Along with a policy, there is a high level \verb|Agent| class that manages the data ingestion and other high-level control. There are 3 main configurations it can be in:
\begin{itemize}
  \item \verb|is_grasp|, meaning the grasping head is separated from the pose and action predictor.
  \item \verb|is_proprio|, allowing proprioceptive data will be included in the pipeline
\end{itemize}
Each of these also have a corresponding \verb|_opts| field, so modules can be fine controlled further down the policy network. They were kept as defaults I landed on during experiments, and will be disclosed when where necessary. Each agent can also be of \emph{RNN} flavour meaning, the final state encoding will be passed through a LSTM network for temporal reasoning.

For the grasping tasks the agents were configured to use \verb|is_grasp| and \verb|is_prorio| only used during proprioception results, and not anytime before.

\subsection{Policy Parameters}
Most policies were trained using the configuration in Table \ref{tab:eval-training-params}. Any other setting that was enabled will be discussed in their respective sections. The \emph{italics} are variables that are inserted during  the test. The \emph{SEED} was locked to \( \left[ 3790, ~1901, ~4248, ~6689, ~7653 \right] \) which was a randomly generated set. The \emph{Epochs} was a variable range depending on the test and policy type.

\begin{table}[ht]
\centering
  \begin{tabular}{|| c | c ||}
  \hline
  Parameter & Value \\
  \hline
  minibatch\_size & 10 \\
  lr & $1\times 10^{-3}$ \\
  shuffle\_data & \texttt{True} \\
  dataset\_to\_use & \texttt{`demo'} \\
  epochs & \emph{Epochs} \\
  lock\_loader\_seed & \emph{SEED} \\
  \hline
  \end{tabular}\caption{Default Training parameters}\label{tab:eval-training-params}
\end{table}