\section{Depth Interfacing}\todo{does this warrant a whole new section or should I talk about it within obs and no-obs and grasp etc}


\subsection{3D Modelling} \todo{this is not done yet, gonna be a long week init}
Following on from the earlier discoveries and proposed hypotheses, before I can start deigning an active vision framework, I had to understand what in traditional static views might be holding a robot back. Continuing from the earlier drawn parallels to humans, we know that to understand where an object is in our field of vision by stereo vision. Having two points of view, allows us to take and process two very slightly poses of objects to reinforce our understanding of where that object is. Other information such as lighting (shadows) and other secondary information may unconsciously help us as well. However, the main takeaway is that understanding distance to an object goes a long way in firstly understanding how to approach an object.

The few ways to achieve this in robotics and RLBench specifically is either to use two cameras (any two works as long as the distance between them and their intrinsic information is known) or use depth sensors, like (light\todo{? actual name}) cameras to have more information about our scenes. We are also provided with point clouds and other 3D niceties, which can bne incorporated in 3D reasoning policies if needed.

\subsection{Exploring the Capabilities of Our Simple Agent}
So, to understand how to reason in 3D, we have to test the capabilities of our agent's understanding of its surroundings and workspace. So, that we can provide either extra information or data where it has gaps in knowledge

\subsubsection{The Setup and The Task}\todo{talk about the grasp policy and the changes made}
The experiment that makes the most sense is a grasping task. As the task performer needs to understand where an object is before it can attempt to grab it. So, I created a modified version of the simple grasping task where the target object's distance and scale can be externally varied to observe the behaviours of the robot.
\missingfigure{pictures of the task}
\todo{create some sort of test suite to test this}

\subsubsection{Creating an Appropriate Policy}
The policy for grasping needed slight modifications compared to the simpler reaching task. Along with understanding what is being seen for movement of the joints, we also needed a mechanism for sending a grasping signal for the robot. The previous policy would regress the entire 6DoF along with the final \emph{float} that controls the gripper. This was not sufficient for this task. Firstly, there weren't many frames the gripper is signaled to be closed, as the gripping happens at the end of the task, comparatively less observations where the gripper is closed; means that there is an inherent imbalance in our dataset. If we were to treat \textbf{CLOSED} and \textbf{OPEN} as binary labels; which they are as RLBench takes a \emph{float} $\in \left[0, 1\right]$ and thresholds at $ > 0.9$ to check if it should be open. Therefore, regressing the entire action with the gripper bit makes it extremely uncertain and mostly skew towards staying open. 

To adjust for this I first tried to do binary classification with weighting the samples to counteract this, which was not fruitful just due to the overall dataset size being small as well. So, going back to regression, I decided to add Cross Entropy Loss and with a gripper prediction head just predict the gripper action from the extracted camera features, then add this as a part of the overall loss with a weighting \todo{add loss in math notation with lambda gripper}. This meant that the gripper action is separately tunable to the movement action. This makes inherent sense as the movement decisions should never affect the gripper. The information flow is camera to movement and gripper action separately \todo{small flowchart to show cam  move, cam gripper action} 


\subsubsection{Incorporating the Depth Camera}
probably cant just fuse with the rgb, might need a new conv for this then merger header