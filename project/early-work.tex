\section{Early Work}
Early steps I was advised to work on was, getting an idea of \emph{learning} in two dimensions (2D) to get an intuition and follow it on with expanding it to three dimensions (3D) from there.

\subsection{Structuring the Problem}
Following those ideas I wanted to create a simple task to train a policy to teach a block to move towards the other block, on simple 2D rendered canvas. Essentially creating a two dimensional reaching task.

\subsubsection{Problem Design} 
I started with pygame \cite{pygame}, a python library that can be used to create 2D canvas with artifacts in it. But before rendering there are some things we have to define. I wanted to create a small\emph{ish} canvas to simulate the game in, with my player and target. I chose to make a $800 \times 600$ canvas and defined my blocks to be $20 \times 20$. Skipping the details the simple game was rendered as Figure \ref{fig:initial-canvas}, where the agent (blue) attempts to get to the target (red).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{assets/early-work/initial-canvas.png}
  \caption{Initial canvas $800 \times 600$ and $20 \times 20$ blocks}\label{fig:initial-canvas}
\end{figure}

As this was a simple game, for movement I defined a speed, which was initially $5$ pixels per press and mapped the movements to the arrow keys, and wrote a simple check that when the agent makes it to the target the game is complete.

\subsection{Solving the Problem: Learning}
To make a policy that would play the game, there was a few things I needed; firstly, the game needed a model to solve, I started with a classificiation approach, to classify the correct keys to press to move to the target.


\subsubsection{Classification}
Therefore, my model was fairly simple; the state, $S$ which is a \emph{4-tuple} $\langle float,~float,~float,~float \rangle$ that holds -in order- $x$-coordinate of agent, $y$-coordinate of agent, $x$-coordinate of target and the $y$-coordinate of target. And due to the keypress nature of the system, the Action, $\mathcal{A}$ was also a \emph{4-tuple} $\langle boolean,~boolean,~boolean,~boolean \rangle$ which was true or false to indicate if an arrow key was pressed, the positions in order are: \emph{Up, Down, Left} and \emph{Right}.

So, knowing the space, I created an expert policy, \(\pi_{demo}\), which gives the key combinations that must be pressed by interpolating where the target is with respect to the agent. Using their coordinates this was pretty simple. So, I randomly generated some starting positions and made the agent play the game many times and got snapshots of the states and stored them in a labelled structure for training a model later. \(data: list\left[State,~Action\right]\) where:
\[
  State:~tuple\left[float,~float,~float,~flaot\right], 
  \hspace{1cm} Action:~tuple\left[bool,~bool,~bool,~bool\right]
\]
as before. (Data loading was managed with dataframes and pickles for optimal GPU loading and speed)

Therefore, combining the two, I trained a linear neural network with 3 layers in PyTorch \cite{pytorch} predict the key presses (the action) given the coordinates of the blocks (state).

Without getting into implementation details, as this is early work, this worked pretty well. Every frame the state would be fed into the model and the model gives out the movement for that frame. Although, the movement was quite jagged because the classification was predicting key presses and the discrete actions were inherently not smooth; because a small change in a direction could prompt the model to change the  direction suddenly. A solution to this would be to get the direction change instead of a discrete movement direction. So, I pivoted to a regression model.

\subsubsection{Regression}
Similarly, as the scope changing I had to change my model slightly. State remained the same, but the action is now defined to have the type: \(Action: tuple[float, float]\), reflecting the change in $x$ and the change in $y$ (\(\delta x , \delta y\)).
Following a similar approach I now had a regression model, trained on the same data (also augmented to have coordinate labels)


\subsection{Extending the Problem: Vision}
To step the investigation in the direction of vision, now I wanted to do the same learning, however, without access to the coordinates of the blocks. I wanted to keep the regression interface, as the smooth movement was working a lot better and, in an ideal robot scenario in the real-world regression based actions are more common due to to continuous values in ranges of motion.

I started by creating my training data, this time as I was making the expert policy play the game, instead of saving the state as a \emph{4-tuple} of coordinates it was taking a screenshot of the canvas. As the canvas is an RGB image, the type of state now became: $State: list[Image, Action]$ where the image is a vector of size $3 \times 800 \times 600$, for the 3 RGB channels and the size of the canvas.

\subsubsection{Receptive Area}
Then I experimented with CNNs to be able to extract information from the scene and infer some sort of relationship. The first issue I faced was with receptive field size, I was mainly using a kernel size of 5 and stride of 2, so that by image was efficiently getting downscaled down the pipeline to learn information at smaller scales. 

However, due to the block size being too small compared to the total size of the canvas, CNNs that weren't deep enough were not performing well, due to not extracting features well. So I decided to increase the size of the blocks to $50 \times 50$ and after some experimentation and tuning parameters I landed on this model (Figure \ref{fig:cnn-5050}) that performed as well as, if not better, than the coordinate based regression model.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/early-work/cnn-diagram.png}
  \caption{One of the best models for the agent reaching task}\label{fig:cnn-5050}
\end{figure}

\subsubsection{Sampling in Phases}
Another issue I ran into was the this model was performing generally well in getting very close to the target but refusing to take a very small step to collide and successfully complete the task. I realised this was because of the random sample collection being generally far away from the target, by the nature of uniform random sampling and there being vastly more space away from the target. So, to fix this, rather than changing the model architecture I changed the way expert data was sampled.

In training, I added a parameter to the training dataset, and to the creation of the training data. I made sure I could control the places data was being generated and labelling them where they were getting generated with respect to the the target. Called these phases of generation and made sure i could control the ratio of training samples and which phase they were coming from during the training. 

See Figure \ref{fig:phase-regions}, for an explanation of this idea. Giving more weighting to a closer region to the target, allowed my model to not suffer from the distribution shifts in my data, and allowed for the final push in the right direction for the agnet to complete the task successfully.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{assets/early-work/regions.png}
  \caption{Illustration of phases around the target, 0 would be green, 1 would be blue and 2 would be the rest of the canvas in this example. }\label{fig:phase-regions}
\end{figure}


\subsubsection{Returning to Small Blocks}
I also then got a model that was only one more convolutional layer deeper to work on the earlier smaller block size, however, the training time increase between the two were quite large for the same data sizes. Around one hour versus, four hours and change, for about 1000 episodes. Therefore, I continued this experimentation with the larger block size.


\subsection{Next Hurdle: Obstacles}
Then the next rational step was to add obstacles to simulate some ablations, which would mimic occlusions in 3D. Although, not visual barriers this was another hurdle the agent had to overcome.
Then the agent uses a path-finding algorithm, inspired by the $A^*$ algorithm \cite{cui2011based}, explores the cells around it with the heuristic of keeping the Euclidian distance of it and the target low.

\subsubsection{Obstacle Generation}
I took a simple procedural checkered approach where the canvas was divided into a grid which has cells the size of the agent. Then a cell will either be an obstacle or not with some clever mechanics around ensuring at least a single path from the agent to the target and some uncertainty. Check Figure \ref{fig:obs-gen} for some examples.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{assets/early-work/obs-gen1.png}
      \caption{Example 1}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{assets/early-work/obs-gen2.png}
      \caption{Example 2}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{assets/early-work/obs-gen3.png}
      \caption{Example 3}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=0.8\linewidth]{assets/early-work/obs-gen4.png}
      \caption{Example 4}
  \end{subfigure}
  \caption{The 2 by 2 grid of some example obstacle generations}\label{fig:obs-gen}
\end{figure}\

\subsubsection{Getting Stuck}
I faced some issues with the model learning to avoid obstacles, the above policy for the no-obstacle case worked pretty well most of the time, however some select cases, such as there being an obstacle right between the agent and the target. 
\todo{figure for this, maybe for final report.} 

After some experimentation, I thought this might be because of the way I modelled the prediction system. Every frame expects a new movement to be generated, per the state given. This inherently has no history about where the agent was moving from. Similarly, the training data is also not ordered in any way, and it is frame corresponds to movement. Therefore, my model was essentially learning the underlying policy of the Euclidean distance heuristic used in the path finding and not extracting useful information about trajectories. This could have been solved with a reinforcement learning approach, or a demonstration requesting apoproach. Where the agent can ask for a new demonstration once it gets stuck or certain amount of time passes \todo{not sure here, talk about new demonstration and why I didn't do this?}

\subsubsection{Future of 2D: Sequence Models}
My next approach was to play around with a model that would take in multiple frames and output either one or multiple moves in order to have this idea of a history.

I experimented briefly with a \emph{sequence-to-movement} model primitively replicating a RNN, where it kept a 10 frame buffer along with the movement to predict the next movement. This idea seemed promising, however, my initial approaches were not fruitful, the models were suffering from a lot of empty episodes or repeated frames were causing erroneous learning. Therefore, I realised I had to pivot to non-concrete length sequences, such as transformer models accepting $n$ number of history frames with a max cap, rather than always needing the $x$ my makeshift-RNN was using (10 in my case). 

This is where I have currently left the 2D experimentation and moved onto starting some 3D approaches.

\section{Transitioning to 3D}
The move to 3D was slow, the jump from \emph{pygame} to a full blown physics simulation was a big leap and it came with its problems.

\subsection{CoppeliaSim and RLBench}

CoppeliaSim is a mostly open source simulation program that provides full access to its features through an educational license and RLBench is an in-house (Imperial) tool adapted to plug into CoppeliaSim through its python API interface (PyRep \cite{}) \todo{cite pyrep}. 

There are many tutorials online to help learn how CoppeliaSim operates and designing some scenes \todo[color=green]{maybe cite some stuff here, not necessary}. However, RLBench is a harder beast to tame. Although the system is very smart and eliminates a lot of the manual work needed to be done before starting experimentation, it was now slightly out of date and I had many issues setting it up on my system.

\subsection{Environment Issues}\todo{entire section needs a reread and structuring}
One of the first large-scale issues I'd face in this project came here, quite early in its lifespan. I would soon learn robotics development is mostly done on Linux based machines and the journey to getting everything working would be quite long.

\subsubsection{Windows Setup}
I do most development work on Windows and WSL (Windows Subsystem for Linux) \cite{} and expecting this project to be fairly memory and graphic power hungry I though I'd setup everything on the Windows side. This caused a few issues with PyRep, this was becuase PyRep firstly expected to be running on Linux, and RLBench was having issues working.

\subsubsection{WSL Setup}
I thought this wouldn't be a problem, as WSL version 2 has been pretty good with GUI applications running on Linux and thought I could run Coppelia on there and still access any displays I might need. The translation layer between the operating systems might cause some slowing but there shouldn't be any major issues issue. Upon configuring everything as the installation guide suggested in RLBench and PyRep repositories. I had few issues linking object files downloaded with CoppeliaSim into the PyRep layer had some issues. After spending a lot of time researching issues, and coming across some online threads with similar issues (in different applications, so nothing was immediately applicable) I decided WSL must have been the problem and decided to move onto the next logical step.
 
\subsubsection{Linux Virtual Machine}
I had previously used Ubuntu a lot and even my WSL instance was running Ubuntu. As RLBench suggested Debian based systems, assuming they also developed it in Ubuntu, I created a Virtual Machine running it. After the entire setup process, I was finally able to get the instance running and finally managed to run one of the examples that gets downloaded when installing RLBench.

However, I got hit by another issues fairly quickly, the rendering of Virtual Box was definitely going through some sort of translation through Windows and then the GPU (not even sure, maybe it was all CPU rendered) however, everything was extremely sluggish, and running the non-primitive examples even crashed my virtual machine instances a few times. At this point I realised I had to settle for the real deal and got to partitioning by storage drive.

\subsubsection{Dual Booting}
I had some issues partitioning the drive Windows was already installed, as it had making that its home and spread all around the disk. I currently had no ways of backing up my data and erasing my disk to completely wipe then partitioning before installing windows again. So, I ripped a spare drive I had in my old laptop and booted Linux form there. After running through the same setup steps, I was finally properly running RLBench. With one caveat, CoppeliaSim constantly complained that I was missing some video encoding binaries, though seemed to work perfectly fine; even when I fixed the binaries it would work but every once in a while pop up saying I was missing them. There were other small annoyances like this throughout the entirety of the experimentation but at least now it was working.

\subsubsection{Campus Machines}
I knew that a solution to all of my problems might have been using a machine at the labs. I thought an issue with that may have been that RLBench requests a lot of binary linking and editing which might have required and the troubleshooting wouldn't be as easy as doing it on my machine. Which could be resolved if I requested a machine to use, but I though my personal machine had the appropriate powerful hardware for a machine learning project and wanted to use that. So jumping through all these steps tool about an entire week, but was well worth it at the end.


\subsection{Usability Issues}
One of the major instability issues I had was because RLBench is about 5 years old now and CoppeliaSim has moved on in some parts, and I was not able to get exactly the same version of the simulator they had as well as exactly the same version of the libraries used when developing RLBench.

\subsubsection{RLBench Codebase}
As I started using RLBench other issues started popping up, PyRep was randomly failing calls to CoppeliaSim due to errors raised in RLBench due to unexpected types and hitting exceptions such as \emph{``Should not be here''} which was frustrating, but the solution was simple. Entirety of the RLBench source code is accessible, so instead of downloading it as a package I forked the repo and started fixing any issues as they started arising. Once trivial issues were getting fixed I also started using CoppeliaSim to make mockup tasks (to be outlined in the future \todo{linking!}) and create networks that would use the simulation to train.

Around the time I started training some primitive models and learning more about these tools, I started hitting weirder errors, like the simulator constantly crashing, getting unexpected observation results (cameras such as overhead cameras  missing their input) and other weird issues. Spending even more time combing though some of the \emph{enourmous} codebase I would sometimes find issues to fix them, but it would routinely break some of the package binaries I had linked and just stop working with no way to reason or figure out how to fix it. At this point I decided to try some other tools.

\subsubsection{PyBullet}
\todo{Left it here last time continue}

\subsection{Toolset Dilemma}\todo{needs a total rewrite this is more of a plan/rough draft}
Faced a lot of issues with rlbench, however, the time I spent on it was too long and learnt how to fix any issues when they came up. It also provided niceties like requesting demos and wiring environment and tasks fairly seamlessly

On the other hand, pybullet was customisable and I think overall ran better, coppeliasim (and especially my installation had a few issues which i couldnt seem to fix) but a lot of the ground work done by rlbench I would need to redo. 

So the main dilemma was, should I be wasting time making a comprehensive suite to fit my needs early on in the project and then cancontinue with the premise of the task, or stick with rlbench and solve issues as they arose. Once I fixed a lot of the OS dependent errors, newer versions of coppelia was for some reason not very happy with newer Ubuntu versions, I decided to stick with rlbench, and hoping any issue arising from this point on shouldn't be a system breaking one, and I was familiar enough with the rlbench codebase to at least attempt to fix anything at this point. So I went back to square one and got the work on rlbench.



\todo{following to be deleted}
I didn't do any meaningful work in terms of experimenting with learning or robots, but spent majority of my time setting up RLBench (quite tricky OS requirements and problems) and playing around to get myself familiar with the software.