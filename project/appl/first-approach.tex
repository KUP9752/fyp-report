\section{First Approach: 3D Reasoning}
Main idea here is to not touch the training for the policy. Which is to say, a policy will be learnt as normal from demonstrations using imitation learning. Then during its execution, the robot can use 3D reasoning and possibly planning to move to its optimal state before applying what its learnt from the demonstration (which are specifically relating to the task)

\subsection{Proposed Approach}
What makes this system is the \emph{optimal pose predictor}, as we want the robot to place itself in a situation where the:
\begin{itemize}
  \item Object visibility is maximised
  \item Occlusions are minimised
\end{itemize}
all the while keeping the robot in a valid kinematic state as dictated by the physics in the simulator.


A simple system would follow the following decision process:
\missingfigure{Train -> evaluate visibility -> if POOR: runu viewpoint optimisation, move gripper there -> execute trained IL policy}

We can formalise this optimal pose system as follows:
\[
p^* = {argmax}_{p \in \mathcal{P}}
  \left[
    \mathcal{V}\left(p; \mathcal{O}\right)
    - 
    \lambda\mathcal{C}\left(p\right)
  \right]
\]

Where \(p^* \in \mathcal{P}\), is a (reachable) candidate gripper pose, represented in in \(\mathbb{R}^7\): \( \left[ x, ~y, ~z, ~qx, ~qy, ~qz, ~qw\right]\), as RLBench stores pose information as a list of 3D coordinates (\(x, ~y, ~z\)) and a quaternion (\(q = \langle x, ~y, ~z, ~w \rangle \)) to represent rotation. Which is a more compact way of representing rotations compared to $3\times3$ matrices.

$\mathcal{O}$ is the given observation. So, any  3D information about the target object and the scene respectively. They can be mesh data about the objects, or possibly point clouds. This approach will initially have to depend on prior knowledge about the target, then later I might be able to encode the target information to be learnt during training so that we can remove the dependence on providing priors.

$\mathcal{V}$ is the visibility score for our target object. We need to define some scoring functions to asses the utility of our current pose within the scene.

And finally, $\mathcal{C}$, is some sort of constraint or cost function to help shape the choices correctly, it might be a constraint to make sure candidate poses are reachable, or maybe a cost function determining whether a candidate pose is too far from the current configuration. $\lambda$ is just a tunable weight to adjust how much these limitations contribute to out optimal pose finding. To add, there maybe multiple cost functions so we can expand them as: \(\lambda_1\mathcal{C}_1 + \cdots +\lambda_n\mathcal{C}_n \).


\subsection{Implementation}
Following the plan, the most important part to start with is our utility function, and a way of sampling poses. Then I can start devising constraints to help shape the policy.

\subsubsection{Scoring a Candidate Pose}
The very first step is to implement an utility function. The initial plan was to use a pretrained segmentation network. Such as MaskRCNN \todo{ref?}. The idea was to segment objects present in the scene and ideally get some bounding boxes of where the system thought the objects were. However, this had some application issues, because these networks were mainly trained with real-life data and pictures the toy tasks that I created without proper graphics overlaying them, would commonly be misclassified or not be classified at all. 

Along with bounding boxes MaskRCNN also gives uncertainties as well as the label of what it thinks this item is. Which I initially thought I could use as a score for `seeing' the target.From the wrist camera it would almost always find the grippers (which are visible) but it was quite uncertain at marking my main target object, and the very rare times it did mark it it would not be certain what it was. From the many pretrained labels it would guess between: `apple', `fire hydrant', and `traffic light'. This is unideal because in the fully autonomous training stage, the network does not necessarily understand about what its looking for, and it doesn't care what its looking for. This however made it hard to quantify the \emph{vision score} with the uncertainty because firstly it couldn't decide on what it was that it was seeing, meaning I couldn't extract just a label from the MaskRCNN return dictionary and check its uncertainty. On top of this it would sometimes latch onto other things in t he scene, such as the grippers I mentioned (mostly identified them as \todo{i cant remember check again?}). So I either had to label these by hand, which is inefficient, or find a way to have the network learn what its target is through an unsupervised (or at least semi-supervised manner).
\todo{add figures of bounding box redwo in rwd copy if needed}

This therefore lead me to pursue a simpler idea implementing a utility function is to extract the target colour and mask the RGB camera to score its visibility (similar to how it was done in `attn' before \todo{link})





\subsubsection{Choosing a Candidate Pose}
Sampling around a sphere from the gripper's pose

\subsubsection{Action Loop}
Talk about the initial system of starting active, and then 

\section{Initial Review}\todo{not sure what to name, did some tests and what not this is ass}

\subsubsection{Collision Avoidance}\todo{talk about the distance back- off, action smoothing}

\subsubsection{Modified action loop}


\todo{add details on how its implemented and maybe code snippets, graph of the spherical sampling would help a lot here}
  