\section{First Approach: 3D Reasoning}
Main idea here is to not touch the training for the policy. Which is to say, a policy will be learnt as normal from demonstrations using imitation learning. Then during its execution, the robot can use 3D reasoning and possibly planning to move to its optimal state before applying what its learnt from the demonstration (which are specifically relating to the task)

\subsection{Proposed Approach}
What makes this system is the \emph{optimal pose predictor}, as we want the robot to place itself in a situation where the:
\begin{itemize}
  \item Object visibility is maximised
  \item Occlusions are minimised
\end{itemize}
all the while keeping the robot in a valid kinematic state as dictated by the physics in the simulator.


A simple system would follow the following decision process:
\missingfigure{Train -> evaluate visibility -> if POOR: runu viewpoint optimisation, move gripper there -> execute trained IL policy}

We can formalise this optimal pose system as follows:
\[
p^* = {argmax}_{p \in \mathcal{P}}
  \left[
    \mathcal{V}\left(p; \mathcal{T}, \mathcal{S}\right)
    - 
    \lambda\mathcal{C}\left(p\right)
  \right]
\]

Where \(p^* \in \mathcal{P}\), is a (reachable) candidate gripper pose, represented in in \(\mathbb{R}^7\): \( \left[ x, ~y, ~z, ~qx, ~qy, ~qz, ~qw\right]\), as RLBench stores pose information as a list of 3D coordinates (\(x, ~y, ~z\)) and a quaternion (\(q = \langle x, ~y, ~z, ~w \rangle \)) to represent rotation. Which is a more compact way of representing rotations compared to $3\times3$ matrices.

$\mathcal{T}$ and $\mathcal{S}$ are the 3D information about the target object and the scene respectively. They can be mesh data about the objects, or possibly point clouds. This approach will initially have to depend on prior knowledge about the target, then later I might be able to encode the target information to be learnt during training so that we can remove the dependence on providing priors.

$\mathcal{V}$ is the visibility score for our target object. We need to define some scoring functions to asses the utility of our current pose within the scene.

And finally, $\mathcal{C}$, is some sort of constraint or cost function to help shape the choices correctly, it might be a constraint to make sure candidate poses are reachable, or maybe a cost function determining whether a candidate pose is too far from the current configuration. $\lambda$ is just a tunable weight to adjust how much these limitations contribute to out optimal pose finding. To add, there maybe multiple cost functions so we can expand them as: \(\lambda_1\mathcal{C}_1 + \cdots +\lambda_n\mathcal{C}_n \).


\subsection{Implementation}
Following the plan, the most important part to start with is our utility function, and a way of sampling poses. Then I can start devising constraints to help shape the policy.

\subsubsection{Scoring a Candidate Pose}
The first, and simplest, idea to start implementing a utility function is to extract the target colour and mask the RGB camera to score its visibility (similar to how it was done in `attn' before \todo{link})

\subsubsection{Choosing a Candidate Pose}


\todo{add details on how its implemented and maybe code snippets and graphs?}
