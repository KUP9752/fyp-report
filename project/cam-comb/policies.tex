\section{Multi-View Understanding Policies}
On top of just naively combining cameras and expecting the policy to understand what it means to `see' is a tall order. The next step is to understand how we can make the most of the information we are given by incorporating the various views and sensors and seeing what combination makes the most sense.

\subsection{Feature Extraction}
So far, I have been using a simple CNN and extracting 

\subsubsection{Better Feature Understanding}


\subsubsection{Simple Stacking}
This is the naive method of just stacking the view information, which is what I have been using up until this point. Simply stacking information on the channel dimension and feeding them into the network. This has clear disadvantages. Firstly, for the RGB cameras, there is definite misalignment, all these cameras have different poses and will disagree on what they see. This will lead to features not lining up leaving us with an non-optimal and more importantly a brittle policy. Meaning slight variations might confuse and move the policy out of distribution.

\subsection{Fusion}
The other level up is fusion of features. The CNNs used can be expanded 
\subsubsection{Late Fusion}

\subsubsection{Mid-Level Fusion}
Have I ever done something on this?? not really maybe talk about how imma use the shoulder stuff with this,


This adds onto before, gated attention and rgb attending to depth etc, explain some approaches

\subsection{Deeper/Better Feature Understanding - RCNN}
Another thing I wanted to try was to include off-the-shelf models to help extract features and information from my views. As one of the problems I described faced earlier was from the agent not remembering the earlier information that it has seen, I believed an important part of increasing the workspace understanding would be to incorporate residual connections.

I experimented with various sizes of ResNet networks \todo[color=green]{reference}. I essentially replaced my own \verb|CNNEncoder| block with modified versions of \emph{resnet18, resnet34, and resnet50} \todo[color=green]{maybe not mention all, afterall the large ones will be ass when the image is so small}. I had to modify these, because the unmodified version using a main CNN of kernel size $7$ was too large and appropriate features were not being extracted from the views I was feeding. This was evident from observing the agent act in a test scenario where the arm would do nothing remotely similar to what the demo did, or even what the task is about.\todo{maybe a picture of the arm doing some bs here}. \todo[color=pink]{run the original confirm it does not work, if not get a diagram of kernel sizes 7 and 3 and an explanation why this might be the case.}

The modified versions, with a smaller kernel, feeding into the later layers and then getting the residual connection didn't seem to work any better either \todo[color=pink]{test on comparing my one with a resnet graph on a toy grasping task, maybe grasp then move?}

A reason these might not have worked well is because of one of my earlier constraints. The image sizes being \(64 \times 64\) pixels, might not be enough to extract meaningful information with a ResNet. This is likely due to its aggressive pooling between the layers and especially during the residual connection and the aggressiveness only increases in larger models. \todo[color=red]{not actually sure if this is right, fact check, wrote this some time ago} 

\subsubsection{increasing the camera view for experimentation}\todo[color=red]{larger image trials? expand the task to be lager 128 or even 224 seems common with resnet}

\subsection{Temporal Understanding of Demonstrations}
Even though the agent is not half bad at trying to reach the target in the above configurations, it is not necessarily better at succeeding at grasping, and the movements seem janky compared to the  precise demonstrations given. I believe this is because the robot is only given its current view and does not understand where it might be during the epsiode \todo{this is the issue I faced}

and the robot doesn't necessarily understand where in the movement it is, because the first half of the episode is very repetitive and the inputs are eerily similar. This meant that an important part of the system was for my agent to understand where it was as well as when it was.

\subsubsection{Latent RNN Encodings of Movement Steps}
I wanted to encode the timestep information into some sort of feature representation which would use the previous states to encode the data. This simple idea was to introduce temporal noise into the decision process. Then act on the final encoding


\missingfigure{diagram about feeding the entire demo and training per step}

\todo{problems here were that the entire demo was being used, but the optimisation was getting made at the final state}

\subsubsection{Fine-Grained per Time-Step Training}
Once I realised my mistake, which was about discarding half my encoded steps and were not using my RNN to its full potential \todo{rewrite} As I have access to ground truth information at every observation step, it would be smart to optimise the loss in a fine-grained manner at every frame.
\missingfigure{updated diagram to convey the padding and unpacking and training the loss on every frame step}
This lead to a more robust and confident architecture. The movements of my policy felt less spiky, no more sudden jerks and more calculated reaching. It did still absolutely overfit to the average position, and was not much better at grasping, but the episodic movements were cleaner and I believe enhancing the grasping branch of the network would allow it to be more successful \todo{what the fuck am i talking about, summarise and clean up}


\subsection{ViT Encodings}
\todo[color=red]{not done much on this, but may be able to talk about how I would go about it or just remove}

\section{Proprioception}\todo{maybe talk about why I left it out earlier in the chapter}
Up until this point, I left proprioceptive data out of the training, mainly because I wanted to train on purely visual feedback to see what it could achieve without any other state information. Secondly, including state information about the robot, would not necessarily immediately help with the 

\section{Moving to \emph{Active Vision}}


\todo{improve the grasping}

\todo{It might be worth scaling everthing to 128 by 128 next week and do a repeat run so that I can at least say that I have done it}

\todo{ruun grasp then move and confirm the suspicions of the policies with no memory failing.}