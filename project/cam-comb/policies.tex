\section{Multi-View Understanding Policies}
On top of just naively combining cameras and expecting the policy to understand what it means to `see' is a tall order. The next step is to understand how we can make the most of the information we are given by incorporating the various views and sensors and seeing what combination makes the most sense.

\subsection{Feature Extraction}
So far, I have been using a simple CNN and extracting 

\subsubsection{Better Feature Understanding}


\subsubsection{Simple Stacking}
This is the naive method of just stacking the views. Simply stacking information on the channel dimension and feeding them into the network. This has clear disadvantages. Firstly, for the RGB cameras, there is definite misalignment, all these cameras have different poses and will disagree on what they see. This will lead to features not lining up, leaving us with a non-optimal and more importantly a brittle policy. This could lead to unintended coupling of information which can lead to the agent making wrong choices.

\subsection{Fusion}
The other level up is fusion of features. The CNNs used can be expanded to either work for a single view or a few of them

\subsubsection{Late Fusion}

\subsubsection{Mid-Level Fusion}
Have I ever done something on this?? not really maybe talk about how imma use the shoulder stuff with this, i guess the film ones can kinda be classified as this?

This adds onto before, gated attention and rgb attending to depth etc, explain some approaches

\subsection{Deeper/Better Feature Understanding - RCNN}\todo{I feel like this section is talking about the wrong thing}
Another thing I wanted to try was to include off-the-shelf models to help extract features and information from my views. As one of the problems I described faced earlier was from the agent not remembering the earlier information that it has seen, I believed an important part of increasing the workspace understanding would be to incorporate residual connections.

I experimented with various sizes of ResNet networks \todo[color=green]{reference}. I essentially replaced my own \verb|CNNEncoder| block with modified versions of \emph{resnet18, resnet34, and resnet50} \todo[color=green]{maybe not mention all, afterall the large ones will be ass when the image is so small}. I had to modify these, because the unmodified version using a main CNN of kernel size $7$ was too large and appropriate features were not being extracted from the views I was feeding. This was evident from observing the agent act in a test scenario where the arm would do nothing remotely similar to what the demo did, or even what the task is about.\todo{maybe a picture of the arm doing some bs here}. \todo[color=pink]{run the original confirm it does not work, if not get a diagram of kernel sizes 7 and 3 and an explanation why this might be the case.}

The modified versions, with a smaller kernel, feeding into the later layers and then getting the residual connection didn't seem to work any better either \todo[color=pink]{test on comparing my one with a resnet graph on a toy grasping task, maybe grasp then move?}

A reason these might not have worked well is because of one of my earlier constraints. The image sizes being \(64 \times 64\) pixels, might not be enough to extract meaningful information with a ResNet. This is likely due to its aggressive pooling between the layers and especially during the residual connection and the aggressiveness only increases in larger models. \todo[color=red]{not actually sure if this is right, fact check, wrote this some time ago} 

\subsubsection{increasing the camera view for experimentation}\todo[color=red]{larger image trials? expand the task to be lager 128 or even 224 seems common with resnet}

\subsection{Temporal Understanding of Demonstrations}
Although, I am preserving the sequential nature of the demonstrations intact in training. There is no implicit underlying mechanism for this. As the scenes and the movements are very repetitive the agent will benefit from reinforcing its understanding of where it is, by learning when it is. So, I will incorporate recurrent models. This is so that the embedding of the current states will on a history of the earlier observations or states.

\subsubsection{Latent RNN Encodings of Movement Steps}
I wanted to encode the timestep information into some sort of feature representation which would use the previous states to encode the data. This simple idea was to introduce temporal noise into the decision process. Then act on the final encoding

\missingfigure{diagram about feeding the entire demo and training per step}

\todo{problems here were that the entire demo was being used, but the optimisation was getting made at the final state}

\subsubsection{Fine-Grained per Time-Step Training}
Once I realised my mistake, which was about discarding half my encoded steps and were not using my RNN to its full potential \todo{rewrite} As I have access to ground truth information at every observation step, it would be smart to optimise the loss in a fine-grained manner at every frame.
\missingfigure{updated diagram to convey the padding and unpacking and training the loss on every frame step}
This lead to a more robust and confident architecture. The movements of my policy felt less spiky, no more sudden jerks and more calculated reaching. It did still absolutely overfit to the average position, and was not much better at grasping, but the episodic movements were cleaner and I believe enhancing the grasping branch of the network would allow it to be more successful \todo{what the fuck am i talking about, summarise and clean up}


\subsection{ViT Encodings}\todo{kind of dabled with this but will not be testing it}
\todo[color=red]{not done much on this, but may be able to talk about how I would go about it or just remove}

\section{Proprioception}\todo{maybe talk about why I left it out earlier in the chapter}
Up until this point, I left proprioceptive data out of the training, mainly because I wanted to train on purely visual feedback to see what it could achieve without any other state information. Secondly, including state information about the robot, would not necessarily immediately help with the 

\section{Moving to \emph{Active Vision}}
End-to-end models with absolutely zero fine-grained interactions are quite competent. However, in the chaotic and random environments an agent may be acting in. Planning and specific nudges may prove beneficial. Similar to the 

\todo{improve the grasping}

\todo{It might be worth scaling everthing to 128 by 128 next week and do a repeat run so that I can at least say that I have done it}

\todo{ruun grasp then move and confirm the suspicions of the policies with no memory failing.}