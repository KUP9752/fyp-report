@Misc{silver2015,
  author = {David Silver},
  title = {Lectures on Reinforcement Learning},
  howpublished = {\textsc{url:}~\url
  {https://www.davidsilver.uk/teaching/}},
  year = {2015}
}


@article{Schmidhuber2015DLandNNreview,
  title={Deep learning in neural networks: An overview},
  volume={61},
  ISSN={0893-6080},
  url={http://dx.doi.org/10.1016/j.neunet.2014.09.003},
  DOI={10.1016/j.neunet.2014.09.003},
  journal={Neural Networks},
  publisher={Elsevier BV},
  author={Schmidhuber, Jürgen},
  year={2015},
  month=jan, pages={85–117} 
}

@article{Pierson18082017,
author = {Harry A. Pierson and Michael S. Gashler},
title = {Deep learning in robotics: a review of recent research},
journal = {Advanced Robotics},
volume = {31},
number = {16},
pages = {821--835},
year = {2017},
publisher = {Taylor \& Francis},
doi = {10.1080/01691864.2017.1365009},


URL = { 
    
        https://doi.org/10.1080/01691864.2017.1365009
    
    

},

}

% data efficiency
@InProceedings{wu23robotLearn,
  title = 	 {DayDreamer: World Models for Physical Robot Learning},
  author =       {Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Abbeel, Pieter and Goldberg, Ken},
  booktitle = 	 {Proceedings of The 6th Conference on Robot Learning},
  pages = 	 {2226--2240},
  year = 	 {2023},
  editor = 	 {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  volume = 	 {205},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14--18 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v205/wu23c/wu23c.pdf},
  url = 	 {https://proceedings.mlr.press/v205/wu23c.html},
  abstract = 	 {To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without any simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place objects from camera images and sparse rewards, approaching human-level teleoperation performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, which establishes a strong baseline. We release our infrastructure for future applications of world models to robot learning.}
}


@ARTICLE{newbury2023graspSynthReview,
  author={Newbury, Rhys and Gu, Morris and Chumbley, Lachlan and Mousavian, Arsalan and Eppner, Clemens and Leitner, Jürgen and Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica and Fox, Dieter and Cosgun, Akansel},
  journal={IEEE Transactions on Robotics}, 
  title={Deep Learning Approaches to Grasp Synthesis: A Review}, 
  year={2023},
  volume={39},
  number={5},
  pages={3994-4015},
  keywords={Grasping;Deep learning;Task analysis;Grippers;Systematics;Shape;Force;Dexterous manipulation;deep learning in robotics and automation;grasping;perception for grasping and manipulation},
  doi={10.1109/TRO.2023.3280597}
}

  
@Article{liu2021DRLminireview,
  AUTHOR = {Liu, Rongrong and Nageotte, Florent and Zanne, Philippe and de Mathelin, Michel and Dresp-Langley, Birgitta},
  TITLE = {Deep Reinforcement Learning for the Control of Robotic Manipulation: A Focussed Mini-Review},
  JOURNAL = {Robotics},
  VOLUME = {10},
  YEAR = {2021},
  NUMBER = {1},
  ARTICLE-NUMBER = {22},
  URL = {https://www.mdpi.com/2218-6581/10/1/22},
  ISSN = {2218-6581},
  ABSTRACT = {Deep learning has provided new ways of manipulating, processing and analyzing data. It sometimes may achieve results comparable to, or surpassing human expert performance, and has become a source of inspiration in the era of artificial intelligence. Another subfield of machine learning named reinforcement learning, tries to find an optimal behavior strategy through interactions with the environment. Combining deep learning and reinforcement learning permits resolving critical issues relative to the dimensionality and scalability of data in tasks with sparse reward signals, such as robotic manipulation and control tasks, that neither method permits resolving when applied on its own. In this paper, we present recent significant progress of deep reinforcement learning algorithms, which try to tackle the problems for the application in the domain of robotic manipulation control, such as sample efficiency and generalization. Despite these continuous improvements, currently, the challenges of learning robust and versatile manipulation skills for robots with deep reinforcement learning are still far from being resolved for real-world applications.},
  DOI = {10.3390/robotics10010022}
}



@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@book{underactuated2023mit,
  title        = "Underactuated Robotics",
  subtitle     = "Algorithms for Walking, Running, Swimming, Flying, and Manipulation",
  howpublished = "Course Notes for MIT 6.832",
  author       = "Tedrake, Russ",
  year         = 2023,
  url          = "https://underactuated.csail.mit.edu",
}

% talking about MFRL and MBRL (and benchmariking MBRL)
@misc{wang2019benchmarkingmodelbasedreinforcementlearning,
      title={Benchmarking Model-Based Reinforcement Learning}, 
      author={Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba},
      year={2019},
      eprint={1907.02057},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.02057}, 
}
% alleviate rl model bias
@misc{kurutach2018modelensembletrustregionpolicyoptimization,
      title={Model-Ensemble Trust-Region Policy Optimization}, 
      author={Thanard Kurutach and Ignasi Clavera and Yan Duan and Aviv Tamar and Pieter Abbeel},
      year={2018},
      eprint={1802.10592},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.10592}, 
}
@misc{chua2018deepreinforcementlearninghandful,
      title={Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}, 
      author={Kurtland Chua and Roberto Calandra and Rowan McAllister and Sergey Levine},
      year={2018},
      eprint={1805.12114},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.12114}, 
}

@misc{clavera2018modelbasedreinforcementlearningmetapolicy,
      title={Model-Based Reinforcement Learning via Meta-Policy Optimization}, 
      author={Ignasi Clavera and Jonas Rothfuss and John Schulman and Yasuhiro Fujita and Tamim Asfour and Pieter Abbeel},
      year={2018},
      eprint={1809.05214},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.05214}, 
}

% reinforcement learning - teaching topaly videogames
@article{comi2018,
    title = "How to Teach an AI to Play Games using Deep Q-Learning",
    author  = "Comi, Mauro",
    journal = "Towards Data Science",
    year    = "2018",   
    url     = "https://towardsdatascience.com/how-to-teach-an-ai-toplay-games-deep-reinforcement-learning-28f9b920440a"
}

%RF use in NLP
@misc{paulus2017deepreinforcedmodelabstractive,
      title={A Deep Reinforced Model for Abstractive Summarization}, 
      author={Romain Paulus and Caiming Xiong and Richard Socher},
      year={2017},
      eprint={1705.04304},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.04304}, 
}
% RF use in healthcare
@misc{yu2020reinforcementlearninghealthcaresurvey,
      title={Reinforcement Learning in Healthcare: A Survey}, 
      author={Chao Yu and Jiming Liu and Shamim Nemati},
      year={2020},
      eprint={1908.08796},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.08796}, 
}
## Behavioural Cloning
@inproceedings{Bain1995AFF,
  title={A Framework for Behavioural Cloning},
  author={Michael Bain and Claude Sammut},
  booktitle={Machine Intelligence 15},
  year={1995},
  url={https://api.semanticscholar.org/CorpusID:10738655}
}

% model based RL
@article{MAL-086,
url = {http://dx.doi.org/10.1561/2200000086},
year = {2023},
volume = {16},
journal = {Foundations and Trends® in Machine Learning},
title = {Model-based Reinforcement Learning: A Survey},
doi = {10.1561/2200000086},
issn = {1935-8237},
number = {1},
pages = {1-118},
author = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker}
}


%% MBRL reduces model bias
@inproceedings{Deisenroth2011PILCO,
  author    = {Marc Deisenroth and Carl E. Rasmussen},
  title     = {{PILCO}: A Model-Based and Data-Efficient Approach to Policy Search},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages     = {465--472},
  year      = {2011}
}


% on policy RL experiments
@misc{andrychowicz2020onpolicyRL,
      title={What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study}, 
      author={Marcin Andrychowicz and Anton Raichuk and Piotr Stańczyk and Manu Orsini and Sertan Girgin and Raphael Marinier and Léonard Hussenot and Matthieu Geist and Olivier Pietquin and Marcin Michalski and Sylvain Gelly and Olivier Bachem},
      year={2020},
      eprint={2006.05990},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.05990}, 
}

% Q-Lrning algorithm
@article{Watkins1992,
  author    = {Christopher J. C. H. Watkins and Peter Dayan},
  title     = {Q-learning},
  journal   = {Machine Learning},
  year      = {1992},
  volume    = {8},
  number    = {3},
  pages     = {279--292},
  doi       = {10.1007/BF00992698},
  url       = {https://doi.org/10.1007/BF00992698},
  issn      = {1573-0565}
}

% exploration and exploitations
@misc{liu2019simpleexplorationsampleefficient,
      title={When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms}, 
      author={Yao Liu and Emma Brunskill},
      year={2019},
      eprint={1805.09045},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.09045}, 
}


@misc{qu2020combiningmodelbasedmodelfreemethods,
      title={Combining Model-Based and Model-Free Methods for Nonlinear Control: A Provably Convergent Policy Gradient Approach}, 
      author={Guannan Qu and Chenkai Yu and Steven Low and Adam Wierman},
      year={2020},
      eprint={2006.07476},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2006.07476}, 
}
%Fitted Qlearning
@InProceedings{10.1007/11564096_32FittedQLearning,
author="Riedmiller, Martin",
editor="Gama, Jo{\~a}o
and Camacho, Rui
and Brazdil, Pavel B.
and Jorge, Al{\'i}pio M{\'a}rio
and Torgo, Lu{\'i}s",
title="Neural Fitted Q Iteration -- First Experiences with a Data Efficient Neural Reinforcement Learning Method",
booktitle="Machine Learning: ECML 2005",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="317--328",
abstract="This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.",
isbn="978-3-540-31692-3"
}




% uses DQN to great success
@article{Mnih2015,
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  title     = {Human-level control through deep reinforcement learning},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  month     = {February},
  doi       = {10.1038/nature14236},
  url       = {https://doi.org/10.1038/nature14236}
}
