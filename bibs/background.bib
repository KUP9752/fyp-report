@Misc{silver2015,
  author = {David Silver},
  title = {Lectures on Reinforcement Learning},
  howpublished = {\textsc{url:}~\url
  {https://www.davidsilver.uk/teaching/}},
  year = {2015}
}

@article{maroti2019rbed,
  title={Rbed: Reward based epsilon decay},
  author={Maroti, Aakash},
  journal={arXiv preprint arXiv:1910.13701},
  year={2019}
}

//TODO: fix this?
@Misc{johns2024,
  author = {Edward Johns},
  title = {Lectures on Robot Learning},
  note = {Accessed: 2025-01-22}
}

@article{vitiello2023one,
  title={One-shot imitation learning: A pose estimation perspective},
  author={Vitiello, Pietro and Dreczkowski, Kamil and Johns, Edward},
  journal={arXiv preprint arXiv:2310.12077},
  year={2023}
}

% VGN
@misc{breyer2021volumetricgraspingnetworkrealtime,
      title={Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter}, 
      author={Michel Breyer and Jen Jen Chung and Lionel Ott and Roland Siegwart and Juan Nieto},
      year={2021},
      eprint={2101.01132},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2101.01132}, 
}

@article{fewshotsurvey,
author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
title = {Generalizing from a Few Examples: A Survey on Few-shot Learning},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3386252},
doi = {10.1145/3386252},
abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {63},
numpages = {34},
keywords = {small sample learning, prior knowledge, one-shot learning, meta-learning, low-shot learning, Few-shot learning}
}


@article{Schmidhuber2015DLandNNreview,
  title={Deep learning in neural networks: An overview},
  volume={61},
  ISSN={0893-6080},
  url={http://dx.doi.org/10.1016/j.neunet.2014.09.003},
  DOI={10.1016/j.neunet.2014.09.003},
  journal={Neural Networks},
  publisher={Elsevier BV},
  author={Schmidhuber, Jürgen},
  year={2015},
  month=jan, pages={85–117} 
}

@article{Pierson18082017,
author = {Harry A. Pierson and Michael S. Gashler},
title = {Deep learning in robotics: a review of recent research},
journal = {Advanced Robotics},
volume = {31},
number = {16},
pages = {821--835},
year = {2017},
publisher = {Taylor \& Francis},
doi = {10.1080/01691864.2017.1365009},


URL = { 
    
        https://doi.org/10.1080/01691864.2017.1365009
    
    

},

}

% data efficiency
@InProceedings{wu23robotLearn,
  title = 	 {DayDreamer: World Models for Physical Robot Learning},
  author =       {Wu, Philipp and Escontrela, Alejandro and Hafner, Danijar and Abbeel, Pieter and Goldberg, Ken},
  booktitle = 	 {Proceedings of The 6th Conference on Robot Learning},
  pages = 	 {2226--2240},
  year = 	 {2023},
  editor = 	 {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  volume = 	 {205},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14--18 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v205/wu23c/wu23c.pdf},
  url = 	 {https://proceedings.mlr.press/v205/wu23c.html},
  abstract = 	 {To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without any simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place objects from camera images and sparse rewards, approaching human-level teleoperation performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, which establishes a strong baseline. We release our infrastructure for future applications of world models to robot learning.}
}


@ARTICLE{newbury2023graspSynthReview,
  author={Newbury, Rhys and Gu, Morris and Chumbley, Lachlan and Mousavian, Arsalan and Eppner, Clemens and Leitner, Jürgen and Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica and Fox, Dieter and Cosgun, Akansel},
  journal={IEEE Transactions on Robotics}, 
  title={Deep Learning Approaches to Grasp Synthesis: A Review}, 
  year={2023},
  volume={39},
  number={5},
  pages={3994-4015},
  keywords={Grasping;Deep learning;Task analysis;Grippers;Systematics;Shape;Force;Dexterous manipulation;deep learning in robotics and automation;grasping;perception for grasping and manipulation},
  doi={10.1109/TRO.2023.3280597}
}

  
@Article{liu2021DRLminireview,
  AUTHOR = {Liu, Rongrong and Nageotte, Florent and Zanne, Philippe and de Mathelin, Michel and Dresp-Langley, Birgitta},
  TITLE = {Deep Reinforcement Learning for the Control of Robotic Manipulation: A Focussed Mini-Review},
  JOURNAL = {Robotics},
  VOLUME = {10},
  YEAR = {2021},
  NUMBER = {1},
  ARTICLE-NUMBER = {22},
  URL = {https://www.mdpi.com/2218-6581/10/1/22},
  ISSN = {2218-6581},
  ABSTRACT = {Deep learning has provided new ways of manipulating, processing and analyzing data. It sometimes may achieve results comparable to, or surpassing human expert performance, and has become a source of inspiration in the era of artificial intelligence. Another subfield of machine learning named reinforcement learning, tries to find an optimal behavior strategy through interactions with the environment. Combining deep learning and reinforcement learning permits resolving critical issues relative to the dimensionality and scalability of data in tasks with sparse reward signals, such as robotic manipulation and control tasks, that neither method permits resolving when applied on its own. In this paper, we present recent significant progress of deep reinforcement learning algorithms, which try to tackle the problems for the application in the domain of robotic manipulation control, such as sample efficiency and generalization. Despite these continuous improvements, currently, the challenges of learning robust and versatile manipulation skills for robots with deep reinforcement learning are still far from being resolved for real-world applications.},
  DOI = {10.3390/robotics10010022}
}

% off policy RL review
@misc{uehara2022reviewoffpolicyevaluationreinforcement,
      title={A Review of Off-Policy Evaluation in Reinforcement Learning}, 
      author={Masatoshi Uehara and Chengchun Shi and Nathan Kallus},
      year={2022},
      eprint={2212.06355},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2212.06355}, 
}

 
@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@book{underactuated2023mit,
  title        = "Underactuated Robotics",
  subtitle     = "Algorithms for Walking, Running, Swimming, Flying, and Manipulation",
  howpublished = "Course Notes for MIT 6.832",
  author       = "Tedrake, Russ",
  year         = 2023,
  url          = "https://underactuated.csail.mit.edu",
}

% talking about MFRL and MBRL (and benchmariking MBRL)
@misc{wang2019benchmarkingmodelbasedreinforcementlearning,
      title={Benchmarking Model-Based Reinforcement Learning}, 
      author={Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba},
      year={2019},
      eprint={1907.02057},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.02057}, 
}
% alleviate rl model bias
@misc{kurutach2018modelensembletrustregionpolicyoptimization,
      title={Model-Ensemble Trust-Region Policy Optimization}, 
      author={Thanard Kurutach and Ignasi Clavera and Yan Duan and Aviv Tamar and Pieter Abbeel},
      year={2018},
      eprint={1802.10592},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.10592}, 
}
@misc{chua2018deepreinforcementlearninghandful,
      title={Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}, 
      author={Kurtland Chua and Roberto Calandra and Rowan McAllister and Sergey Levine},
      year={2018},
      eprint={1805.12114},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.12114}, 
}

@misc{clavera2018modelbasedreinforcementlearningmetapolicy,
      title={Model-Based Reinforcement Learning via Meta-Policy Optimization}, 
      author={Ignasi Clavera and Jonas Rothfuss and John Schulman and Yasuhiro Fujita and Tamim Asfour and Pieter Abbeel},
      year={2018},
      eprint={1809.05214},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.05214}, 
}

% reinforcement learning - teaching topaly videogames
@article{comi2018,
    title = "How to Teach an AI to Play Games using Deep Q-Learning",
    author  = "Comi, Mauro",
    journal = "Towards Data Science",
    year    = "2018",   
    url     = "https://towardsdatascience.com/how-to-teach-an-ai-toplay-games-deep-reinforcement-learning-28f9b920440a"
}

%RF use in NLP
@misc{paulus2017deepreinforcedmodelabstractive,
      title={A Deep Reinforced Model for Abstractive Summarization}, 
      author={Romain Paulus and Caiming Xiong and Richard Socher},
      year={2017},
      eprint={1705.04304},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.04304}, 
}
% RF use in healthcare
@misc{yu2020reinforcementlearninghealthcaresurvey,
      title={Reinforcement Learning in Healthcare: A Survey}, 
      author={Chao Yu and Jiming Liu and Shamim Nemati},
      year={2020},
      eprint={1908.08796},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.08796}, 
}
## Behavioural Cloning
@inproceedings{Bain1995AFF,
  title={A Framework for Behavioural Cloning},
  author={Michael Bain and Claude Sammut},
  booktitle={Machine Intelligence 15},
  year={1995},
  url={https://api.semanticscholar.org/CorpusID:10738655}
}
% explortation in RL - good for sparse rewards 
@article{LADOSZ20221,
  title = {Exploration in deep reinforcement learning: A survey},
  journal = {Information Fusion},
  volume = {85},
  pages = {1-22},
  year = {2022},
  issn = {1566-2535},
  doi = {https://doi.org/10.1016/j.inffus.2022.03.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253522000288},
  author = {Pawel Ladosz and Lilian Weng and Minwoo Kim and Hyondong Oh},
  keywords = {Deep reinforcement learning, Exploration, Intrinsic motivation, Sparse reward problems},
  abstract = {This paper reviews exploration techniques in deep reinforcement learning. Exploration techniques are of primary importance when solving sparse reward problems. In sparse reward problems, the reward is rare, which means that the agent will not find the reward often by acting randomly. In such a scenario, it is challenging for reinforcement learning to learn rewards and actions association. Thus more sophisticated exploration methods need to be devised. This review provides a comprehensive overview of existing exploration approaches, which are categorised based on the key contributions as: reward novel states, reward diverse behaviours, goal-based methods, probabilistic methods, imitation-based methods, safe exploration and random-based methods. Then, unsolved challenges are discussed to provide valuable future research directions. Finally, the approaches of different categories are compared in terms of complexity, computational effort and overall performance.}
}



% model based RL
@article{MAL-086,
url = {http://dx.doi.org/10.1561/2200000086},
year = {2023},
volume = {16},
journal = {Foundations and Trends® in Machine Learning},
title = {Model-based Reinforcement Learning: A Survey},
doi = {10.1561/2200000086},
issn = {1935-8237},
number = {1},
pages = {1-118},
author = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker}
}


%% MBRL reduces model bias
@inproceedings{Deisenroth2011PILCO,
  author    = {Marc Deisenroth and Carl E. Rasmussen},
  title     = {{PILCO}: A Model-Based and Data-Efficient Approach to Policy Search},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages     = {465--472},
  year      = {2011}
}


% on policy RL experiments
@misc{andrychowicz2020onpolicyRL,
      title={What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study}, 
      author={Marcin Andrychowicz and Anton Raichuk and Piotr Stańczyk and Manu Orsini and Sertan Girgin and Raphael Marinier and Léonard Hussenot and Matthieu Geist and Olivier Pietquin and Marcin Michalski and Sylvain Gelly and Olivier Bachem},
      year={2020},
      eprint={2006.05990},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.05990}, 
}

% Q-Lrning algorithm
@article{Watkins1992,
  author    = {Christopher J. C. H. Watkins and Peter Dayan},
  title     = {Q-learning},
  journal   = {Machine Learning},
  year      = {1992},
  volume    = {8},
  number    = {3},
  pages     = {279--292},
  doi       = {10.1007/BF00992698},
  url       = {https://doi.org/10.1007/BF00992698},
  issn      = {1573-0565}
}

% exploration and exploitations
@misc{liu2019simpleexplorationsampleefficient,
      title={When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms}, 
      author={Yao Liu and Emma Brunskill},
      year={2019},
      eprint={1805.09045},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.09045}, 
}


@misc{qu2020combiningmodelbasedmodelfreemethods,
      title={Combining Model-Based and Model-Free Methods for Nonlinear Control: A Provably Convergent Policy Gradient Approach}, 
      author={Guannan Qu and Chenkai Yu and Steven Low and Adam Wierman},
      year={2020},
      eprint={2006.07476},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2006.07476}, 
}
%Fitted Qlearning
@InProceedings{10.1007/11564096_32FittedQLearning,
author="Riedmiller, Martin",
editor="Gama, Jo{\~a}o
and Camacho, Rui
and Brazdil, Pavel B.
and Jorge, Al{\'i}pio M{\'a}rio
and Torgo, Lu{\'i}s",
title="Neural Fitted Q Iteration -- First Experiences with a Data Efficient Neural Reinforcement Learning Method",
booktitle="Machine Learning: ECML 2005",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="317--328",
abstract="This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.",
isbn="978-3-540-31692-3"
}




% uses DQN to great success
@article{Mnih2015,
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  title     = {Human-level control through deep reinforcement learning},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  month     = {February},
  doi       = {10.1038/nature14236},
  url       = {https://doi.org/10.1038/nature14236}
}
%% ======= Imitation

@inproceedings{bakker1996robot,
  title={Robot see, robot do: An overview of robot imitation},
  author={Bakker, Paul and Kuniyoshi, Yasuo and others},
  booktitle={AISB96 Workshop on Learning in Robots and Animals},
  volume={5},
  pages={3--11},
  year={1996}
}

@article{HUANG2023104691,
title = {To imitate or not to imitate: Boosting reinforcement learning-based construction robotic control for long-horizon tasks using virtual demonstrations},
journal = {Automation in Construction},
volume = {146},
pages = {104691},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104691},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005611},
author = {Lei Huang and Zihan Zhu and Zhengbo Zou},
keywords = {Construction robot, Reinforcement learning, Virtual reality},
abstract = {Construction robots controlled using reinforcement learning (RL) have recently emerged, showing higher adaptability and self-learning intelligence over pre-programmed and teleoperated robots. This work aims to train RL-based construction robots to learn long-horizon tasks with few exploration steps. We propose an approach that collects expert demonstrations in virtual reality, which are then added in the RL loop to assist with learning long-horizon construction tasks. Additionally, we utilize a hierarchical training strategy to generalize control policies to new policies that handle complex scenarios. For evaluation, we implement the approach for picking and installing window panels in simulation. In experiments, all 10 agents trained with virtual demonstrations delivered the task with success rates over 95%. Moreover, these 10 agents generalized their control policies and handled randomized window panels with success rates over 90%. The results confirm the effectiveness of our approach in boosting construction robots’ performance over long-horizon tasks.}
}


% LfD
@article{ARGALL2009469,
title = {A survey of robot learning from demonstration},
journal = {Robotics and Autonomous Systems},
volume = {57},
number = {5},
pages = {469-483},
year = {2009},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2008.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0921889008001772},
author = {Brenna D. Argall and Sonia Chernova and Manuela Veloso and Brett Browning},
keywords = {Learning from demonstration, Robotics, Machine learning, Autonomous systems},
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.}
}

% supervised learning
@article{hastie2009overview,
  title={Overview of supervised learning},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  journal={The elements of statistical learning: Data mining, inference, and prediction},
  pages={9--41},
  year={2009},
  publisher={Springer}
}

@incollection{cunningham2008supervised,
  title={Supervised learning},
  author={Cunningham, P{\'a}draig and Cord, Matthieu and Delany, Sarah Jane},
  booktitle={Machine learning techniques for multimedia: case studies on organization and retrieval},
  pages={21--49},
  chapter = {2},
  year={2008},
  publisher={Springer}
}

% IL explanation
@misc{attia2018globaloverviewimitationlearning,
      title={Global overview of Imitation Learning}, 
      author={Alexandre Attia and Sharone Dayan},
      year={2018},
      eprint={1801.06503},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1801.06503}, 
}

% DAgger
@misc{ross2011reductionimitationlearningstructured,
      title={A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}, 
      author={Stephane Ross and Geoffrey J. Gordon and J. Andrew Bagnell},
      year={2011},
      eprint={1011.0686},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1011.0686}, 
}

% IL BC stuff
@article{pomerlau1991neco.1991.3.1.88,
    author = {Pomerleau, Dean A.},
    title = {Efficient Training of Artificial Neural Networks for Autonomous Navigation},
    journal = {Neural Computation},
    volume = {3},
    number = {1},
    pages = {88-97},
    year = {1991},
    month = {03},
    abstract = {The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN is a backpropagation network designed to drive the CMU Navlab, a modified Chevy van. This paper describes the training techniques that allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching the reactions of a human driver. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, and multilane lined and unlined roads, at speeds of up to 20 miles per hour.},
    issn = {0899-7667},
    doi = {10.1162/neco.1991.3.1.88},
    url = {https://doi.org/10.1162/neco.1991.3.1.88},
    eprint = {https://direct.mit.edu/neco/article-pdf/3/1/88/812106/neco.1991.3.1.88.pdf},
}



%IL survey good stuff (in the folder as well)
@misc{zare2023surveyimitationlearningalgorithms,
      title={A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges}, 
      author={Maryam Zare and Parham M. Kebria and Abbas Khosravi and Saeid Nahavandi},
      year={2023},
      eprint={2309.02473},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.02473}, 
}


%controlling covaraite shift in BC
@misc{mehta2024stablebccontrollingcovariateshift,
      title={Stable-BC: Controlling Covariate Shift with Stable Behavior Cloning}, 
      author={Shaunak A. Mehta and Yusuf Umut Ciftci and Balamurugan Ramachandran and Somil Bansal and Dylan P. Losey},
      year={2024},
      eprint={2408.06246},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2408.06246}, 
}


%% reward is more transferrable to policy
@inproceedings{russell1998learning,
  title={Learning agents for uncertain environments},
  author={Russell, Stuart},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={101--103},
  year={1998}
}


@article{ARORA2021103500,
title = {A survey of inverse reinforcement learning: Challenges, methods and progress},
journal = {Artificial Intelligence},
volume = {297},
pages = {103500},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103500},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000515},
author = {Saurabh Arora and Prashant Doshi},
keywords = {Reinforcement learning, Reward function, Learning from demonstration, Generalization, Learning accuracy, Survey},
abstract = {Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the extant literature in IRL, this article serves as a comprehensive reference for researchers and practitioners of machine learning as well as those new to it to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article surveys a vast collection of foundational methods grouped together by the commonality of their objectives, and elaborates how these methods mitigate the challenges. We further discuss extensions to the traditional IRL methods for handling imperfect perception, an incomplete model, learning multiple reward functions and nonlinear reward functions. The article concludes the survey with a discussion of some broad advances in the research area and currently open research questions.}
}

%offline Rl

@misc{levine2020offlinereinforcementlearningtutorial,
      title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}, 
      author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
      year={2020},
      eprint={2005.01643},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.01643}, 
}

% gail in IL
@misc{ho2016generativeadversarialimitationlearning,
      title={Generative Adversarial Imitation Learning}, 
      author={Jonathan Ho and Stefano Ermon},
      year={2016},
      eprint={1606.03476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.03476}, 
}
GAN networks
@misc{goodfellow2014generativeadversarialnetworks,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1406.2661}, 
}


% active SLAM drone
@Article{drones6040085,
AUTHOR = {Gupta, Abhishek and Fernando, Xavier},
TITLE = {Simultaneous Localization and Mapping (SLAM) and Data Fusion in Unmanned Aerial Vehicles: Recent Advances and Challenges},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {4},
ARTICLE-NUMBER = {85},
URL = {https://www.mdpi.com/2504-446X/6/4/85},
ISSN = {2504-446X},
ABSTRACT = {This article presents a survey of simultaneous localization and mapping (SLAM) and data fusion techniques for object detection and environmental scene perception in unmanned aerial vehicles (UAVs). We critically evaluate some current SLAM implementations in robotics and autonomous vehicles and their applicability and scalability to UAVs. SLAM is envisioned as a potential technique for object detection and scene perception to enable UAV navigation through continuous state estimation. In this article, we bridge the gap between SLAM and data fusion in UAVs while also comprehensively surveying related object detection techniques such as visual odometry and aerial photogrammetry. We begin with an introduction to applications where UAV localization is necessary, followed by an analysis of multimodal sensor data fusion to fuse the information gathered from different sensors mounted on UAVs. We then discuss SLAM techniques such as Kalman filters and extended Kalman filters to address scene perception, mapping, and localization in UAVs. The findings are summarized to correlate prevalent and futuristic SLAM and data fusion for UAV navigation, and some avenues for further research are discussed.},
DOI = {10.3390/drones6040085}
}

% hand eye coord a/synch vision-action
@article{divyaHandEyeCoordsination,
author = {Srinivasan, Divya and Martin, Bernard},
year = {2010},
month = {06},
pages = {391-405},
title = {Eye-hand coordination of symmetric bimanual reaching tasks: Temporal aspects},
volume = {203},
journal = {Experimental brain research. Experimentelle Hirnforschung. Expérimentation cérébrale},
doi = {10.1007/s00221-010-2241-3}
}


% active SLAM
@Article{s23198097,
AUTHOR = {Ahmed, Muhammad Farhan and Masood, Khayyam and Fremont, Vincent and Fantoni, Isabelle},
TITLE = {Active SLAM: A Review on Last Decade},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {19},
ARTICLE-NUMBER = {8097},
URL = {https://www.mdpi.com/1424-8220/23/19/8097},
PubMedID = {37836928},
ISSN = {1424-8220},
ABSTRACT = {This article presents a comprehensive review of the Active Simultaneous Localization and Mapping (A-SLAM) research conducted over the past decade. It explores the formulation, applications, and methodologies employed in A-SLAM, particularly in trajectory generation and control-action selection, drawing on concepts from Information Theory (IT) and the Theory of Optimal Experimental Design (TOED). This review includes both qualitative and quantitative analyses of various approaches, deployment scenarios, configurations, path-planning methods, and utility functions within A-SLAM research. Furthermore, this article introduces a novel analysis of Active Collaborative SLAM (AC-SLAM), focusing on collaborative aspects within SLAM systems. It includes a thorough examination of collaborative parameters and approaches, supported by both qualitative and statistical assessments. This study also identifies limitations in the existing literature and suggests potential avenues for future research. This survey serves as a valuable resource for researchers seeking insights into A-SLAM methods and techniques, offering a current overview of A-SLAM formulation.},
DOI = {10.3390/s23198097}
}




% in the wild IL
@misc{bahl2022humantorobotimitationwild,
      title={Human-to-Robot Imitation in the Wild}, 
      author={Shikhar Bahl and Abhinav Gupta and Deepak Pathak},
      year={2022},
      eprint={2207.09450},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2207.09450}, 
}


% camera models
@incollection{zhang2021cameramodels,
  title={Camera Model},
  author={Zhang, Zhengyou},
  booktitle={Computer vision: A reference guide},
  pages={131--135},
  year={2021},
  publisher={Springer}
}


%camera history, used for fish-eyee lens
@book{king1989history,
  author    = {King, R. W. and S. D.},
  title     = {History of Photographic Lenses},
  year      = {1989},
  publisher = {Academic Press},
  address   = {Boston},
  language  = {English},
  isbn      = {0124086403},
  url       = {https://archive.org/details/historyofphotogr0000king},
  note      = {Includes bibliographical references (p. 19-21).}
}

% pinhole math
@book{solem2012programming,
  title={Programming Computer Vision with Python: Tools and algorithms for analyzing images},
  author={Solem, Jan Erik},
  year={2012},
  publisher={" O'Reilly Media, Inc."}
}

@misc{mphy0026camera,
  author = "{University College London}",
  title = "Camera Calibration",
  year = "n.d.",
  url = "https://mphy0026.readthedocs.io/en/latest/calibration/camera_calibration.html",
  note = "Accessed: 2025-01-23"
}


@article{Din2014ProjectorCalibration,
  author    = {Din, I. and Anwar, H. and Syed, I. and Zafar, H. and Hasan, L.},
  title     = {Projector Calibration for Pattern Projection Systems},
  journal   = {Journal of Applied Research and Technology. JART},
  volume    = {12},
  number    = {1},
  pages     = {80--86},
  year      = {2014},
  doi       = {10.1016/S1665-6423(14)71608-6},
  url       = {https://www.elsevier.es/es-revista-journal-applied-research-technology-jart-81-articulo-projector-calibration-for-pattern-projection-S1665642314716086},
  issn      = {1665-6423}
}


@article{occlusionimage,
author = {Bechlioulis, Charalampos and Vlantis, Panagiotis and Kyriakopoulos, Kostas},
year = {2021},
month = {05},
pages = {75},
title = {Coordination of Multiple Robotic Vehicles in Obstacle-Cluttered Environments},
volume = {10},
journal = {Robotics},
doi = {10.3390/robotics10020075}
}

%stereo vision
@article{hamzah2016literature,
  title={Literature survey on stereo vision disparity map algorithms},
  author={Hamzah, Rostam Affendi and Ibrahim, Haidi},
  journal={Journal of Sensors},
  volume={2016},
  number={1},
  pages={8742920},
  year={2016},
  publisher={Wiley Online Library}
}

% ToF
@ARTICLE{foix2011tof,
  author={Foix, Sergi and Alenya, Guillem and Torras, Carme},
  journal={IEEE Sensors Journal}, 
  title={Lock-in Time-of-Flight (ToF) Cameras: A Survey}, 
  year={2011},
  volume={11},
  number={9},
  pages={1917-1926},
  keywords={Cameras;Pixel;Sensors;Calibration;Measurement uncertainty;Systematics;Temperature measurement;Calibration;lock-in;time-of-flight (ToF)},
  doi={10.1109/JSEN.2010.2101060}}

% tof and structured light
@article{zanuttigh2016time,
  title={Time-of-flight and structured light depth cameras},
  author={Zanuttigh, Pietro and Marin, Giulio and Dal Mutto, Carlo and Dominio, Fabio and Minto, Ludovico and Cortelazzo, Guido Maria and others},
  journal={Technology and Applications},
  volume={978},
  number={3},
  year={2016},
  publisher={Springer}
}


% edge detection theory
@article{marr1980theory,
  title={Theory of edge detection},
  author={Marr, David and Hildreth, Ellen},
  journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume={207},
  number={1167},
  pages={187--217},
  year={1980},
  publisher={The Royal Society London}
}

% canny edge
@article{canny1986computational,
  title={A computational approach to edge detection},
  author={Canny, John},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={679--698},
  year={1986},
  publisher={Ieee}
}

% harris corner
@article{derpanis2004harris,
  title={The harris corner detector},
  author={Derpanis, Konstantinos G},
  journal={York University},
  volume={2},
  number={1},
  pages={2},
  year={2004}
}

% sift and variants
@article{wu2013comparative,
  title={A Comparative Study of SIFT and its Variants},
  author={Wu, Jian and Cui, Zhiming and Sheng, Victor S and Zhao, Pengpeng and Su, Dongliang and Gong, Shengrong},
  journal={Measurement science review},
  volume={13},
  number={3},
  pages={122--131},
  year={2013}
}

% ORB
@inproceedings{rublee2011orb,
  title={ORB: An efficient alternative to SIFT or SURF},
  author={Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
  booktitle={2011 International conference on computer vision},
  pages={2564--2571},
  year={2011},
  organization={Ieee}
}

%template matchin CV
@misc{brunelli2009template,
  title={Template Matching Techniques in Computer Vision: Theory and Practice},
  author={Brunelli, R},
  year={2009},
  publisher={John Wiley \& Sons}
}

% sift and surf
@article{juan2009comparison,
  title={A comparison of sift, pca-sift and surf},
  author={Juan, Luo and Gwun, Oubong},
  journal={International Journal of Image Processing (IJIP)},
  volume={3},
  number={4},
  pages={143--152},
  year={2009},
  publisher={Citeseer}
}

% ========= CNN
@article{li2021survey,
  title={A survey of convolutional neural networks: analysis, applications, and prospects},
  author={Li, Zewen and Liu, Fan and Yang, Wenjie and Peng, Shouheng and Zhou, Jun},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={12},
  pages={6999--7019},
  year={2021},
  publisher={IEEE}
}

@article{gu2018recent,
  title={Recent advances in convolutional neural networks},
  author={Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and others},
  journal={Pattern recognition},
  volume={77},
  pages={354--377},
  year={2018},
  publisher={Elsevier}
}

@article{traore2018deep,
  title={Deep convolution neural network for image recognition},
  author={Traore, Boukaye Boubacar and Kamsu-Foguem, Bernard and Tangara, Fana},
  journal={Ecological informatics},
  volume={48},
  pages={257--268},
  year={2018},
  publisher={Elsevier}
}

@article{hijazi2015using,
  title={Using convolutional neural networks for image recognition},
  author={Hijazi, Samer and Kumar, Rishi and Rowen, Chris and others},
  journal={Cadence Design Systems Inc.: San Jose, CA, USA},
  volume={9},
  number={1},
  year={2015}
}

% fast rcnn
@misc{girshick2015fastrcnn,
      title={Fast R-CNN}, 
      author={Ross Girshick},
      year={2015},
      eprint={1504.08083},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1504.08083}, 
}

@article{bharati2020deep,
  title={Deep learning techniques—R-CNN to mask R-CNN: a survey},
  author={Bharati, Puja and Pramanik, Ankita},
  journal={Computational Intelligence in Pattern Recognition: Proceedings of CIPR 2019},
  pages={657--668},
  year={2020},
  publisher={Springer}
}

@inproceedings{bi2021transformer,
  title={Transformer in computer vision},
  author={Bi, Jiarui and Zhu, Zengliang and Meng, Qinglong},
  booktitle={2021 IEEE International conference on computer science, electronic information engineering and intelligent control technology (CEI)},
  pages={178--188},
  year={2021},
  organization={IEEE}
}

% DC-ViT
@article{kayacan2024vision,
  title={A Vision Transformer Based Approach to Clutter Removal in GPR: DC-ViT},
  author={Kayacan, Yavuz Emre and Erer, Isin},
  journal={IEEE Geoscience and Remote Sensing Letters},
  year={2024},
  publisher={IEEE}
}


% tempral consistency
@misc{lai2018learningblindvideotemporal,
      title={Learning Blind Video Temporal Consistency}, 
      author={Wei-Sheng Lai and Jia-Bin Huang and Oliver Wang and Eli Shechtman and Ersin Yumer and Ming-Hsuan Yang},
      year={2018},
      eprint={1808.00449},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1808.00449}, 
}

@inproceedings{billington2007using,
  title={Using temporal consistency to improve robot localisation},
  author={Billington, David and Estivill-Castro, Vlad and Hexel, Ren{\'e} and Rock, Andrew},
  booktitle={RoboCup 2006: Robot Soccer World Cup X 10},
  pages={232--244},
  year={2007},
  organization={Springer}
}

@inproceedings{yang2021reactive,
  title={Reactive human-to-robot handovers of arbitrary objects},
  author={Yang, Wei and Paxton, Chris and Mousavian, Arsalan and Chao, Yu-Wei and Cakmak, Maya and Fox, Dieter},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3118--3124},
  year={2021},
  organization={IEEE}
}

% self supervised learning
@inproceedings{lim2022real2sim2real,
  title={Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting},
  author={Lim, Vincent and Huang, Huang and Chen, Lawrence Yunliang and Wang, Jonathan and Ichnowski, Jeffrey and Seita, Daniel and Laskey, Michael and Goldberg, Ken},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)},
  pages={8282--8289},
  year={2022},
  organization={IEEE}
}

@article{huang2021robot,
  title={Robot gaining accurate pouring skills through self-supervised learning and generalization},
  author={Huang, Yongqiang and Wilches, Juan and Sun, Yu},
  journal={Robotics and Autonomous Systems},
  volume={136},
  pages={103692},
  year={2021},
  publisher={Elsevier}
}

% navigation and mapping
@inproceedings{suriani2021s,
  title={S-AVE: Semantic active vision exploration and mapping of indoor environments for mobile robots},
  author={Suriani, Vincenzo and Kaszuba, Sara and Sabbella, Sandeep R and Riccio, Francesco and Nardi, Daniele},
  booktitle={2021 European Conference on Mobile Robots (ECMR)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}


human robot interaction
@ARTICLE{breazeal2001hri,
  author={Breazeal, C. and Edsinger, A. and Fitzpatrick, P. and Scassellati, B.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans}, 
  title={Active vision for sociable robots}, 
  year={2001},
  volume={31},
  number={5},
  pages={443-453},
  keywords={Robot vision systems;Robot kinematics;Humanoid robots;Robot sensing systems;Human robot interaction;Machine vision;Animation;Visual system;Cameras;Artificial intelligence},
  doi={10.1109/3468.952718}}


  % NBV attention mechanisms
  @article{Burusa_2024,
   title={Attention-driven next-best-view planning for efficient reconstruction of plants and targeted plant parts},
   volume={246},
   ISSN={1537-5110},
   url={http://dx.doi.org/10.1016/j.biosystemseng.2024.08.002},
   DOI={10.1016/j.biosystemseng.2024.08.002},
   journal={Biosystems Engineering},
   publisher={Elsevier BV},
   author={Burusa, Akshay K. and van Henten, Eldert J. and Kootstra, Gert},
   year={2024},
   month=oct, pages={248–262} }


   @misc{wang2024observeactasynchronousactive,
      title={Observe Then Act: Asynchronous Active Vision-Action Model for Robotic Manipulation}, 
      author={Guokang Wang and Hang Li and Shuyuan Zhang and Yanhong Liu and Huaping Liu},
      year={2024},
      eprint={2409.14891},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2409.14891}, 
}

@misc{natarajan2021graspsynthesisnovelobjects,
      title={Grasp Synthesis for Novel Objects Using Heuristic-based and Data-driven Active Vision Methods}, 
      author={Sabhari Natarajan and Galen Brown and Berk Calli},
      year={2021},
      eprint={2104.11372},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2104.11372}, 
}


%% audio and vis RL for active vision
@INPROCEEDINGS{rothbucher2011,
  author={Rothbucher, Martin and Denk, Christian and Diepold, Klaus},
  booktitle={2012 IEEE International Workshop on Haptic Audio Visual Environments and Games (HAVE 2012) Proceedings}, 
  title={Robotic gaze control using reinforcement learning}, 
  year={2012},
  volume={},
  number={},
  pages={83-88},
  keywords={Robots},
  doi={10.1109/HAVE.2012.6374444}}

  @article{zhangembodied,
  title={Embodied Memory Through Gaze Control},
  author={Zhang, Ruiyi and Pitkow, Xaq and Angelaki, Dora E}
}

% active vision survey
@misc{placed2023surveyactivesimultaneouslocalization,
      title={A Survey on Active Simultaneous Localization and Mapping: State of the Art and New Frontiers}, 
      author={Julio A. Placed and Jared Strader and Henry Carrillo and Nikolay Atanasov and Vadim Indelman and Luca Carlone and José A. Castellanos},
      year={2023},
      eprint={2207.00254},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2207.00254}, 
}

% probabilistic robotics
@article{thrun2002probabilistic,
  title={Probabilistic robotics},
  author={Thrun, Sebastian},
  journal={Communications of the ACM},
  volume={45},
  number={3},
  pages={52--57},
  year={2002},
  publisher={ACM New York, NY, USA}
}

@misc{dhami2023prednbvpredictionguidednextbestview3d,
      title={Pred-NBV: Prediction-guided Next-Best-View for 3D Object Reconstruction}, 
      author={Harnaik Dhami and Vishnu D. Sharma and Pratap Tokekar},
      year={2023},
      eprint={2304.11465},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2304.11465}, 
}

@misc{breyer2022closedloopnextbestviewplanningtargetdriven,
      title={Closed-Loop Next-Best-View Planning for Target-Driven Grasping}, 
      author={Michel Breyer and Lionel Ott and Roland Siegwart and Jen Jen Chung},
      year={2022},
      eprint={2207.10543},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2207.10543}, 
}