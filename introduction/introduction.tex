\chapter{Introduction}
\section{Motivation}
    
  % //TODO add more papers to where cameras are placed as I read more papers

  Reinforcement and Imitation Learning are the two most popular ways for robots to learn new tasks. In such scenarios they are usually fitted with cameras that allows them to judge and ``see'' their environment. These cameras can be mounted in many configurations: Mounted on their wrists \cite{chi2024UMIinthewild,openXEmbodimentRoboticLearning2024}, over the shoulder \cite{??}, or in some cases placed around the environment \cite{?} that the robot is interacting to allow the entire scene to be observed. And these are not necessarily mutually exclusive, one can combine one or more of the mounting systems \cite{exploringActiveVision2024chuang} all in efforts to ensure the robot while collecting data and learning a task has the most unobstructed and complete information in order to ``understand'' scenes.

  % Maybe give some references to stuff read here about where the cameras are mounted in different projects

  On this visual information we can train control policies for robots using Neural Networks \cite{spyros1995nnStateOfTheArt, Schmidhuber2015nn} -of differing architectures- that are fit for the task we need our robot to do. However, these methods of mounting cameras comes with various disadvantages:
  
  \textbf{Static Cameras} Environment observing static cameras, such as over-the-shoulder mounted cameras \cite{??} mean that the robot is confined to its environment and occlusions are still possible depending on the angles and the number of such cameras placed.
    
  \textbf{Camera-in-hand} This method can suffer from similar issues. As the gripper is oriented to execute a task, the viewpoints the camera is exposed to will change. This dependency means that select tasks that are inherently occluding, once the gripper is engaged; for example, screwing in a lightbulb \textbf{give some other examples}. This means that optimal learning cannot be achieved for such a robot. As we get partial observability of the Region of Interest (ROI) at best and complete occlusion of the ROI at worst.
    
  In contrast to the above, us humans use our eyes which can be operated independently from our arms. This optimising aspect of human visual feedback \cite{findlay2003active,maiello2021humans,goodman2018using} is one of the cornerstones of human learning and and environment manipulation. So, a possible remedy of the above issues is having a free moving, controllable camera alongside the gripper. This human-like setup -with the grippers as the arms and the camera arm as the neck, head and eyes- now allows our robot to find optimal camera poses for the tasks at hand and gain the most information about a scene.
  
  % //TODO (??)Talk about hand engineered tasks maybe?
  This now means that the visumotor tasks shift to that of an \emph{Active Vision} one. This is a highly active area of research in robotics and mostly tackled through synchronous video with teleoperation \cite{exploringActiveVision2024chuang,?} with Behavioural Cloning \cite{?}, where a human ``pilot'' will control the robot through some sort of interface, like a virtual reality (VR) headset. So the robot can learn to move its  

  \todo{make this work or remove} Literature generally seems to explore IL/RL separately to active vision, meaning active vision is mostly tested on already trained robots, or at least those with prior knowledge about the task. Write a paragraph on this and cite those papers

  So our goal in this project is to explore how a robot can learn end-to-end control policies while also learning an active vision policy to optimise the movement of the camera during execution of tasks. And how this compares to existing methods of learning policies. \todo{with/without active vision or with different kinds of active vision}

\section{Objectives}
    % NOT sure about the hints in the parentheses
    So, the plan of this project then is to investigate whether a robot can be made to learn an active vision policy around the task it is currently being instructed (Imitation) or is currently exploring with some reward (Reinforcement).

    %% //NOTE: This section might completely disappear depending on where we get to, I want to first find a way to say that moving the camera is indeed useful and helps to make the robot learn certain tasks better.
    We are therefore proposing three ways to investigate this:
    \begin{enumerate}
      \item Robot learns a policy as normal. Then during policy execution, the robot uses 3D-reasoning and forward planning, in order to predict its optimal pose so that the object(s) of interest are clearly visible and not occluded by other objects.
      \item The robot randomly moves its camera around during the policy learning in order to generate a range of different camera viewpoints (for the same robot states). When the policy is deployed, the uncertainty of the policy \textbf{(?? Using an ensemble method)} can be used to guide the camera towards regions with low uncertainty.
      \item In simulation, reinforcement learning is used to train a policy that directly controls the camera, for any given task and object, using randomly generated "pseudo-tasks". Then after training a robot on a task using human demonstrations in the real world. Two policies will be deployed: the main policy controlling the robot's hand, and the policy controlling the robot's camera.
\end{enumerate}

Following on from the results of a predetermined test suite we can then determine what is the most promising model out of the three and develop them further.
\section{Scope}
% //TODO:
\todo{Not sure if this is needed for Interim report, will come back}

\begin{itemize}
	\item Types of tasks the project will cover
	\item assumptions, camera, robot, arm types etc
	\item predetermined test suite maybe?
	\item distinction between control policies for the hand arm (policy for task) and the camera (optimal view)
\end{itemize}

% \section{Contributions}
